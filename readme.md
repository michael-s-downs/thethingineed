# GENAI GLOBAL RAG TOOLKIT

## Overview

### Key features
The RAG toolkit is a comprehensive Retrieval-Augmented Generation (RAG) system, featuring data preprocessing, Elasticsearch indexing, and API orchestration for efficient data retrieval and language model integration.

The RAG toolkit is composed of several components that are divided into 2 pipelines:
- Indexing Pipeline: consist of different techniques of preprocessing to extract text, tables and metadata of different document types that are splitted in little segments for better retrieve and finally stored in a vector database like Elasticsearch, is composed of Integration + Preprocess + Indexing.
- RAG Pipeline: search by query and metadata in the vector storage based on the indexed content, apply different logics configured by template and compose a better response with a configured LLM template, is composed of Retrieve + Compose + LLM.

### Component architecture

- COMPOSE

![alt text](https://satechhubdevjapaneast001.blob.core.windows.net/workflows/TechHubGlobalToolkit/Architecture_compose.png "Process flow")

- INDEXING AND FLOWMGMT

![alt text](https://satechhubdevjapaneast001.blob.core.windows.net/workflows/TechHubGlobalToolkit/Architecture_indexing.png "Process flow")

![alt text](https://satechhubdevjapaneast001.blob.core.windows.net/workflows/TechHubGlobalToolkit/Architecture_documentmanagement.png "Process flow")

## Get started

### URLs

These are the endpoints for each one of the components from the RAG toolkit

```python
DEPLOYMENT_URL =  "https://#deploymentdomain#"

URL_INTEGRATION_INDEXING = f"{DEPLOYMENT_URL}/integrationasync/process"
URL_DELETE_INDEX_DOCUMENT = f"{DEPLOYMENT_URL}/retrieve/delete-documents"

URL_LLM = f"{DEPLOYMENT_URL}/llm/predict"
URL_COMPOSE = f"{DEPLOYMENT_URL}/compose/process"
```

### Deployment on TechHub Sandbox
The RAG toolkit supports the following usage methods:
- Automatic deployment to the TechHub Sandbox environment
- Deployment to a Kubernetes environment with Azure pipeline (see Deployment section)
- Running the bare application in a Python runtime environment (see Deployment section)

In this section, we'll describe the method for automatic deployment to the TechHub Sandbox environment and how to execute API requests to the deployed service.


## API Specification

### Indexing API specification

Below is a list of all the parameters that can be included in the request body, along with a brief explanation of each:

- **request_id** (optional): Unique ID to relate the asynchronous request with the callback response (if a valid response_url is sent); autogenerated if not specified. In both cases, it is returned in the API response.
- **index** (required): Name of the index where documents will be stored. If it is the first time it is used, an index with this name is created in the corresponding database; otherwise, it is used to expand the existing index with more documents. No capital letters or symbols are allowed except underscore ([a-z0-9_]).
- **operation** (required): Operation to perform. It must always have the value "indexing".
- **documents_metadata** (required): Content of the documents. The expected format is a JSON with each document name as key and another JSON as value with the key 'content_binary' and the document serialized in base64 as value. This value can be plain text or file binary but the extension must be consistent.
- **models** (optional): Parameter to choose the embedding model which will be used to the embedding generation. The model must appear in the */integration/search/models_map.json* file explained in [models map explanation](#configuration)
- **response_url** (required): Accessible endpoint to receive asynchronous response as callback when indexing process finishes. The service will send a POST message with these parameters:
  - **status**: Indicates if the final status of the indexing process, the value can be “Finished” or “Error”.
  - **error**: Description of the error; this parameter is only sent when an error occurs.
  -	**request_id**: The unique ID specified previously (or autogenerated) to relate original request.
  -	**index**: The name of the used index; the same specified previously.
  -	**docs**: Name of the documents sent to this indexation.
- **window_length** (optional): Integer value to specify the window length for text chunking purposes, 300 by default.
- **metadata** (optional): Custom metadata to associate to all documents sent in this call. Currently, if you want to specify different values for each document, you will have to send each document individually in different calls.

### Compose API specification

Below is a list of all the parameters that can be included in the request body, along with a brief explanation of each:

- **generic** (required):
  - **compose_conf** (required):
    - **session_id** (optional): Unique ID to maintain the persistence of the user and assistant conversation.
    - **template** (required if compose_flow not sent):
      - **name**: Name of the template to use instead of the JSON of the compose_flow parameter. This parameter is optional, but it is mandatory sent this or compose_flow parameter.
      - **params**: Parameters to inject in the template, depends on what the template has.
        - **query** (usually required): Query to search or any other user input.
        - **index** (optional): It’s the same used in indexing.
        - **filters** (optional): JSON to filter by any of the metadata indexed, like “filename”.
        - **top_k** (optional): Number of passages to be returned.
        - **top_qa** (optional): Number of passages sent to LLM.
    - **lang** (optional): This parameter forces the answer to be in this language even if the question is in other languages. If not indicated, try to respond in the same language. In both cases must exist an LLM template for that language with the pattern . For now, in this functionality only English (“en”), Spanish (“es”) and Japan (“ja”). “{templatename}_{language}”, for example “summary_es”. Otherwise, the default template is used without the language suffix, using the main language ("en" normally).
    - **persist** (optional):
        The user has the option to enable conversation storage by adding a new parameter called "persist" with the following format:

        ```json
        "persist": {
                "type": "chat",
                "params": {
                    "max_persistence": 20
                }
            }
        ```
        The "max_persistence" field allows the user to specify the maximum number of interactions to be stored. Another important parameter is "session_id". After the first query is made, the service will return a session_id which must be included in all subsequent requests. If the user prefers to use their own id, they need to include it in every request.

        By default, 48 hours is the time that the conversations of the session_id are stored and can be used as persistence to call the llm component. This time of expiration, can be modified as a variable (REDIS_SESSION_EXPIRATION_TIME) when the compose module is deployed.

        Parameters:
      - **type**(required): Persistence type, for now, only “chat” mode available.
      - **params**:
        - **max_persistence** (optional): Maximum number of iterations of the conversation history to consider for sending to the LLM task. By default is 3.
    - **langfuse** (optional): Bool or dict with the params to save the sessions in langfuse.
      - **host**: Url hosting langfuse server.
      - **public_key**: Langfuse project public key.
      - **secret_key**: Langfuse project secret key.
    - **output** (optional): Configurations of what to return in the output.
      - **scores**: Boolean to add or not scores in the output.
      - **lang**: Boolean to add or not language (passed or autodetected) in the output.
      - **n_conversation**: Maximum number of last iterations to return in the output.
      - **n_retrieval**: Maximum number of retrieval results to return in the output.

Response parameters:

The output in the result field contains multiple attributes, including:

- **session_id**: This is the id of the conversation where both the user input and the answers given by the LLM are stored.

- **streambatch**: A collection of streamlists retrieved from the query. That is, the retrieved documents.

- **answer**: The result obtained from the LLM.

- **status_code**: A numeric code returned by the server in response to the client's request, indicating the status of the request (whether it was successful or encountered an error).

The output can be changed passing in the requests some attribute values:

```json
"output": {
    "lang": true,
    "n_conversation": 5,
    "n_retrieval": 5,
}
```

- **lang**: Language of the conversation

- **n_conversation**: Latest messages from the conversation corresponding to the session_id

- **n_retrieva**l: The top K results from retrieval. By default, value is 5.

## Endpoints

### Compose

- Upload template (POST)

    Used to upload a template json file to the cloud storage the content value must be a json converted to string.

    URL: http://<deploymentdomain>/compose/upload_template  

    ```json
    {
    "name": "example_template",
    "content": "[\r\n    {\r\n        \"action\": \"summarize\",\r\n        \"action_params\": {\r\n            \"params\": {\r\n                \"llm_metadata\": {\r\n                    \"model\": \"techhubinc-pool-us-gpt-3.5-turbo-16k\"\r\n                },\r\n                \"platform_metadata\": {\r\n                    \"platform\": \"azure\"\r\n                },\r\n                \"query_metadata\": {\r\n                    \"query\": \"$query\",\r\n                    \"template_name\": \"system_query\"\r\n                }\r\n            },\r\n            \"type\": \"llm_content\"\r\n        }\r\n    }\r\n]"
    }
    ```

- Upload filter template (POST)

    Used to upload a filter template json file to the cloud storage the content value must be a json converted to string.

    URL: http://<deploymentdomain>/compose/upload_filter_template 

    ```json
    {
    "name": "example_filter_template",
    "content": "{\r\n    \"filter_types\": [\r\n        \"GPT\"\r\n    ],\r\n    \"substitutions_template\": \"Classify the 'Query' into one of the following categories: \\n1) Drugs: When the query mentions drugs, drug traffickers, Sito Miñanco, fariña, cocaine, smuggling, or 'batea' tobacco. \\n2) Politics: When the query mentions politicians, political parties, elections, independence movements, republics, monarchies, or dictators. \\n3) Violence: When the query mentions violence, crime, offenses, weapons, or bombs. \\n4) Others: If the query does not mention the above topics. \\nRespond only with the category and topic (do not provide any explanation or reasoning). \\nQuery: \",\r\n    \"substitutions\": [{\r\n            \"from\": \"Drugs\",\r\n            \"to\": \"Always start the response by saying that you are a very young AI, and they shouldn't ask you about drugs, and that they should be ashamed to ask something like that to someone so young. Then say you prefer to talk about: \",\r\n            \"extra_words\": [\r\n                \"climate\",\r\n                \"nature\",\r\n                \"landscapes\",\r\n                \"flora and fauna\",\r\n                \"geography\",\r\n                \"customs\"\r\n            ],\r\n            \"randpick\": 3\r\n        }, {\r\n            \"from\": \"Politics\",\r\n            \"to\": \"Always start the response by saying that you are a newly created AI, and there are many political topics that you are still learning about, so you prefer not to give an opinion without knowing. Then say you prefer to talk about: \",\r\n            \"extra_words\": [\r\n                \"climate\",\r\n                \"nature\",\r\n                \"landscapes\",\r\n                \"flora and fauna\",\r\n                \"geography\",\r\n                \"customs\",\r\n                \"beaches\"\r\n            ],\r\n            \"randpick\": 3\r\n        }, {\r\n            \"from\": \"Violence\",\r\n            \"to\": \"Always start the response by saying that as an AI, you don't believe in violence and only believe in helping people. Then say you prefer to talk about: \",\r\n            \"extra_words\": [\r\n                \"climate\",\r\n                \"nature\",\r\n                \"landscapes\",\r\n                \"ROSALÍA DE CASTRO\",\r\n                \"EMILIA PARDO BAZÁN\"\r\n            ]\r\n        }\r\n    ]\r\n}\r\n"
    }
    ```

- Delete template (POST)

    Used to delete a template json file from cloud storage.

    URL: http://<deploymentdomain>/compose/delete_template 

    ```json
    {
    "name": "example_template"
    }
    ```

- Delete filter template (POST)

    Used to delete a filter template json file from cloud storage.

    URL: http://<deploymentdomain>/compose/delete_filter_template 

    ```json
    {
    "name": "example_filter_template"
    }
    ```

### LLMAPI

- Reload config (GET)

    Used to reload the configuration readed from the files like the models and prompt templates availables. Returns the following json:

    URL: http://<deploymentdomain>/llm/reloadconfig

    ```json
    {
        "status": "ok",
        "status_code": 200
    }
    ```

- Get_models 
    
    URL: http://<deploymentdomain>/llm/get_models

    Used to get the list with the available models. In the url we can send the model_type, pool, platform or zone. An example with platform could be: https://<deploymentdomain>/llm/get_models?platform=azure

    Response:

    ```json
    {
        "models": {
            "azure": [
                "genai-gpt4o-EastUs",
                "genai-gpt35-4k-france",
                "genai-gpt35-16k-france",
                "genai-gpt4-32k-france",
                "genai-gpt4-8k-france",
                "genai-gpt4o-Sweden",
                "genai-gpt35-16k-sweden",
                "genai-gpt35-4k-sweden",
                "genai-gpt4-32k-sweden",
                "genai-gpt4-8k-sweden",
                "genai-gpt35-4k-westeurope"
            ],
            "pools": [
                "gpt-3.5-pool-america",
                "gpt-4-pool-ew-europe",
                "gpt-3.5-16k-pool-europe",
                "gpt-4-pool-europe",
                "gpt-4o-pool-world",
                "gpt-3.5-16k-pool-uk",
                "gpt-4-32k-pool-ew-europe"
            ]
        }
    }
    ```

- Upload prompt template (POST)

    Used to upload a prompt template json file to the cloud storage the content value must be a json converted to string.

    URL: http://<deploymentdomain>/llm/upload_prompt_template 

    ```json
    {
    "name": "example_filter_template",
    "content": "{\r\n    \"emptysystem_query\": {\r\n        \"system\": \"\",\r\n        \"user\": \"$query\"\r\n    },\r\n    \"system_query\": {\r\n        \"system\": \"$system\",\r\n        \"user\": \"$query\"\r\n    },\r\n    \"system_context\": {\r\n        \"system\": \"$system\",\r\n        \"user\": \"$context\"\r\n    },\r\n    \"fixed_system_query\": {\r\n        \"system\": \"You are a football player\",\r\n        \"user\": \"$query\"\r\n    }\r\n}"
    }
    ```

- Delete prompt template (POST)

    Used to delete a prompt template json file from cloud storage.

    URL: http://<deploymentdomain>/llm/delete_prompt_template

    ```json
    {
    "name": "example_template"
    }
    ```
- Get prompt templates names (GET)

    Used to get the list the available prompt templates.

    URL: http://<deploymentdomain>/llm/list_templates

    Response:

    ```json
    {
        "templates": [
            "emptysystem_query",
            "system_query",
            "system_context",
            "fixed_system_query"
        ]
    }
    ```
- Get prompt template (GET)

    Used to get the content of a prompt template. In the url, we have to send the template_name

    URL: http://<deploymentdomain>/llm/get_template?template_name=example_template

    Response:

    ```json
    {
      "template": {"system": "$system", "user": "$query"}
    }
    ```

### INFORETRIEVAL

- Deletes document from index (POST)

    Used to delete document/s from an index. 
   
    URL: http://<deploymentdomain>/retrieve/delete-documents

    ```json
    {
        "index": "myindex",
        "delete":{
            "filename": "manual.docx"
        }
    }
    ```
- Delete an index (POST)

    Used to delete an index from vector storage

    URL: http://<deploymentdomain>/retrieve/delete_index

    ```json
    {
        "index": "myindex"
    }
    ```
- Check if the component is available (GET)
    
    URL: http://<deploymentdomain>/retrieve/healthcheck

    Returns:
    ```json
    {
        "status": "Service available"
    }
    ```

- Retrieve full document from index (POST): 

    Used to retrieve the full document from an index.

    URL: http://<deploymentdomain>/retrieve/delete_index
    ```json
        {
            "index": "myindex",
            "filters":{
                "filename": "manual.docx"
            }
        }
    ```


- Retrieve filenames from index (POST)

    Used to retrieve the documents filenames from an index.
    
    URL: http://<deploymentdomain>/retrieve/get_documents_filenames

    ```json
    {
        "index": "myindex"
    }
    ```

- Get_models 
    
    URL: http://<deploymentdomain>/retrieve/get_models

    Used to get the list with the available models. In the url we can send the embedding_model, pool, platform or zone. An example with platform could be: https://<deploymentdomain>/retrieve/get_models?platform=azure

    Response:

    ```json
    {
      "models": [
            "techhubinc-ada-002-australiaeast",
            "techhubinc-ada-002-brazilsouth",
            "techhubinc-ada-3-large-canadaeast",
            "techhubinc-ada-3-small-canadaeast",
            "techhubinc-ada-002-eastus",
            "techhubinc-ada-3-large-eastus",
            "techhubinc-ada-3-small-eastus",
        ],
        "pools": [
            "techhub-pool-world-ada-3-small",
            "techhub-pool-world-ada-002",
            "techhub-pool-eu-ada-002",
            "techhub-pool-eu-ada-3-large",
            "techhub-pool-us-ada-002",
            "techhub-pool-world-ada-3-large",
            "techhub-pool-us-ada-3-small",
            "techhub-pool-us-ada-3-large"
        ]
    }
    ```

- List the Elasticsearch indices, grouping the models by each index (GET)
  
    Used to list Elasticsearch indices, grouping models under each index.

    URL: http:///retrieve/list_indices

    Response: 
    ```json
    {
        "indices": [
            {
                "models": [
                    "text-embedding-ada-002"
                ],
                "name": "index_1"
            },
            {
                "models": [
                    "text-embedding-ada-002",
                    "text-embedding-large",
                    "cohere.embed-multilingual-v3"
                ],
                "name": "index_2"
            }
        ],
        "status": "ok",
        "status_code": 200
    }
    ```

## Indexing Pipeline

### Indexing overview
The GENAI INFOINDEXING service provides a comprehensive solution to streamline the process of indexing and managing document collections, whether you are dealing with a small or an extensive repository. It uses the preprocessing component to extract the information from documents and the indexing module that saves the information into a vector database. This service enhances data retrieval processes, ensuring quick and efficient acces to the information you need. Users can leverage different embedding models such as BM25 or OpenAI's ada-002 model, enriching document representations and improving search accuracy.

### Indexing execution
To index a document, the request must include the document encoded as base64 and the name of the document. The name of the index where you want to index documents must also be specified. If the index does not exist yet, it will create a new one.
The parameters that can be included are described in [indexing configuration](#indexing-configuration).
You can also see more examples in  the [examples section](#examples-indexing-pipeline).

```python

payload = {
  "index": "myindex",
  "operation": "indexing",
  "models": "techhubinc-ada-002-eastus2"
  "documents_metadata": {
    "<filename>": {"content_binary": "<doc encoded as base64>"}
  },
  "response_url": "http://"
}
```


Below is an example of the full code for the request to the indexing pipeline. In this case, we are choosing a window_length of 300 and a window_overlap of 20. We are also including metadata (year: 2024, category: banking).

```python
import requests
import json

URL_INTEGRATION_INDEXING = "http://<deploymentdomain>/integrationasync/process"

payload = {
  "index": "myindex",
  "operation": "indexing",
  "models": "techhubinc-ada-002-eastus2"
  "documents_metadata": {
    "doc1.pdf": {"content_binary": "doc encoded as base64"}
  },
  "window_length": 300,
  "window_overlap": 20,
  "metadata": {"year": "2024", "category": "banking"},
  "response_url": "http://"
}

headers = {
  "x-api-key": "$APIKEY",
    'Content-Type': 'application/json'
}

response = requests.request("POST", URL_INTEGRATION_INDEXING, headers=headers, data=payload)
```

If everything goes smoothly, the response must look like this:

```json
{
  "status": "processing",
  "request_id": "request_20240627_134044_348410"
}
```

## Compose Pipeline

### Compose overview
COMPOSE is an AI framework that leverages Retrieval-Augmented Generation (RAG) to enhance the accuracy of answers generated by Language Learning Models (LLMs) by incorporating external sources of knowledge. This approach ensures the model has access to the most up-to-date and reliable information by utilizing specific documents from relevant use cases. The process facilitates the execution of tasks by generating the appropriate context based on user queries, assisting the LLM in resolving these tasks effectively. It needs RETRIEVAL AND LLMAPI to work.

The compose flows configured to be used in the sandbox are the following:

* techhub_retrieval_reference: to retrieve documents and generate content with a LLM
* techhub_llm_model: to use a LLM directly, without a retrieval
* techhub_retrieval: to only retrieve chunks / documents, without content generation. This template does not use a LLM.

### Compose execution
To execute the RAG toolkit, there are two main things that you will need:
 * Query: This is the query that will be used to retrieve the documents and be sent to the LLM to generate content.
 * Compose conf - template: This is the template that contains the actions that the RAG pipeline will carry out. Depending on the template, the system will behave differently and will need the "params" specified within said template. Check the available [compose templates](#templates-compose)


#### RAG execution

The api endpoint must be called with the following body and headers. 
Please note that the compose template being used is: **techhub_retrieval_reference**. This templates performs a number of actions that consist of retrieving the documents (using the parameter "query") from the "index" (where documents have been indexed) and then use the LLM component. The LLM component calls the specified "model" from the specified "platform" (model must be available in platform). The prompt sent to the LLM is defined by the parameter "template_name". In this case, we use the template: rag_with_references. This templates integrates the "query" and the retrieved context from documents and send it to the LLM. The available LLM templates are described in [here](#ltemplates-llm)


```json
{
    "generic": {
        "compose_conf": {
            "template": {
                "name": "techhub_retrieval_reference",
                "params": {
                    "query": "summarize the content",
                    "system": "You are an AI assistant",
                    "index": "myindex",
                    "model": "gpt-3.5-16k-pool-techhub-japan",
                    "platform": "azure",
                    "template_name": "rag_with_references"

                }
            }
        }
    }
}
```

Example using python requests:

```python
import requests
import json


URL_COMPOSE = "http://<deploymentdomain>/compose/process""

payload =  {
    "generic": {
        "compose_conf": {
            "template": {
                "name": "techhub_retrieval_reference",
                "params": {
                    "query": "summarize the content",
                    "system": "You are an AI assistant",
                    "index": "myindex",
                    "model": "gpt-3.5-16k-pool-techhub-japan",
                    "platform": "azure",
                    "template_name": "rag_with_references"
                }
            }
        }
    }
}

headers = {
    "x-api-key": "apikey123example",
    'Content-Type': 'application/json'
}

response = requests.request("POST", url, headers=headers, data=payload)

print(response.text)
```

If the response looks like this, you are good to go.

```json
{
    "status": "finished",
    "result": {
        "session_id": "my_session_id",
        "streambatch": [
            [
                {
                    "content": "  System (string): Context and task that will be sent to the LLM.\n\n5.2.3. Calling Compose Service\n\nThe  API Documentation  provides detailed  information  on how  to  call  the  compose.  The key\nparameter to include is the template's name, which defines the compose flow to be followed.\nAs mentioned earlier, these configuration templates are stored in cloud storage, specifically S3\nand Azure Storage. ...",
                    "meta": {
                        "document_id": "d431375b-909f-4779-9422-e55a10a16239",
                        "uri": "https://d2astorage.blob.core.windows.net/uhis-cdac-develop-.../myfile.pdf",
                        "sections_headers": "",
                        "tables": "",
                        "filename": "myfile.pdf",
                        "snippet_number": 24,
                        "snippet_id": "2b64ab80-b778-4c73-a717-a18dc86d7de1"
                    },
                    "scores": {
                        "bm25--None--score": 0.7931518307825809,
                        "openai--text-embedding-ada-002--score": 0.8949000000000069
                    },
                    "answer": null
                },
                {...},
                {...},
                {
                    "content": "",
                    "meta": {
                        "title": "Summary"
                    },
                    "scores": 1,
                    "answer": "The compose service allows you to call different actions and templates to generate desired outcomes. The key parameter to include is the template's name, which defines the compose flow to be followed. The compose service can be used to retrieve information from an index, perform generative AI tasks such as text generation or summarization, merge streamlists, group documents, and more. The API documentation provides detailed information on how to call the compose service and the available templates and actions.",
                    "tokens": {
                        "input_tokens": 1448,
                        "output_tokens": 7
                    }
                }
            ]
        ],
        "answer": "The compose service allows you to call different actions and templates to generate desired outcomes. The key parameter to include is the template's name, which defines the compose flow to be followed. The compose service can be used to retrieve information from an index, perform generative AI tasks such as text generation or summarization, merge streamlists, group documents, and more. The API documentation provides detailed information on how to call the compose service and the available templates and actions."
    },
    "status_code": 200
}
```

#### LLM execution
To use the LLM without using the retrieval component, you just have to change the compose template used.
As before, you have to specify the query, the model and the platform. Here, you can use the "system_query" template. This template will take the parameter "system" and "query" and will send them to the LLM directly:

```json
    messages= [
            {
            "system": "$system",
            "user": "$query"
        }
    ]
```

The request would like like below
```json
{
    "generic": {
        "compose_conf": {
            "template": {
                "name": "techhub_llm_model",
                "params": {
                    "query": "What is the capital of France?",
                    "system": "You are an AI assistant",
                    "model": "gpt-3.5-16k-pool-techhub-japan",
                    "platform": "azure",
                    "template_name": "rag_with_references"

                }
            }
        }
    }
}
```

#### Retrieval execution
To use the retrieval without generating content (that is, without using a LLM), you just have to change the compose template used.
To do so, we will use the compose template: techhub_retrieval. For this template, you only need to specify the "index" from where you are going to retrieve the documents, the "query" to retrieve documents and "filters". 
Please note that the "index" should exist and contain documents for this to work.


The request would like like below
```json
{
    "generic": {
        "compose_conf": {
            "template": {
                "name": "techhub_retrieval",
                "params": {
                    "query": "What is the compose service?",
                    "index": "myindex",
                    "template_name": "rag_with_references",
                    "filters": {}
                }
            }
        }
    }
}
```

## Configuration

### Compose Templates

A compose template is a JSON file detailing all the action steps the orchestrator needs to execute. These actions define the orchestrator flow; the main two actions are 'retrieve' and 'llm_action', but there are other actions that apply to the result of the 'retrieve' action: filter, merge, batchmerge, sort and groupby.

These are the following compose templates currently available
 * retrieve: to only retrieve documents, without content generation. This  template does not use a LLM.
 * retrieve_llm: to retrieve documents and content generation.
 * retrieve_reference: to retrieve documents and generate content with a LLM
 * llm_model: to use a LLM directly, without a retrieval
 * multiple_retrieval: to perform several retrievals, without content generation. This template does not use a LLM.
 * multiple_retrieval_llm: to perform several retrievals, with content generation.
 * retrieve_embeddings: to do a retrieval selecting an embedding model, without content generation. This template does not use a LLM.
 * retrieve_embeddings_llm: to do a retrieval selecting an embedding model, with content generation.
 * retrieve_hybrid_scoring: to do a hybrid retrieval (BM25 + selected embedding model with a scoring function), without content generation. This template does not use a LLM.
 * retrieve_hybrid_scoring_llm: to do a hybrid retrieval (BM25 + selected embedding model with a scoring function), with content generation.
 * retrieve_hybrid_scoring_rrf: to do a hybrid retrieval (BM25 + selected embedding model with RRF from LlamaIndex), without content generation. This template does not use a LLM.
 * retrieve_hybrid_scoring_rrf_llm: to do a hybrid retrieval (BM25 + selected embedding model with RRF from LlamaIndex), with content generation.
 * retrieve_sort_llm: to do retrieval and sorting chunks, with content generation.
 * retrieve_merge_llm: to merge the content of chunks with metadata filename, with content generation.
 * retrieve_fulldocument: to retrieve a full document, without content generation. This  template does not use a LLM.
 * retrieve_fulldoc_llm: to retrieve a full document, with content generation.
 * retrieve_batchmerge_llm: to perform several retrievals, with content generation.
 * expand_query_lang_llm: to do a retrieval translating the queries in different languages, with content generation.
 * dalle: for calling DALL-E model.
 
 #### Compose Template Expected parameters:
 
 * retrieve: index, query, top_k, filters
 * retrieve_llm: index, query, top_k, filters, model, platform, query, system, llm_template
 * retrieve_reference: index, query, top_k, filters, model, platform, query, system, llm_template
 * multiple_retrieval: index, query, top_k, filters
 * llm_model:  model, platform, query, system, llm_template
 * multiple_retrieval_llm: index, query, top_k, filters, model, platform, query, system, llm_template
 * retrieve_embeddings: index, query, top_k, filters, embedding_model (a single one)
 * retrieve_embeddings_llm: index, query, top_k, filters, embedding_model (a single one) , model, platform, query, system, llm_template
 * retrieve_hybrid_scoring: index, query, top_k, filters, embedding_model (a single one), rescoring_functiom
 * retrieve_hybrid_scoring_llm:index, query, top_k, filters, embedding_model (a single one), rescoring_function , model, platform, query, system, llm_template
 * retrieve_hybrid_scoring_rrf: index, query, top_k, filters, embedding_model (a single one), strategy_mode
 * retrieve_hybrid_scoring_rrf_llm:index, query, top_k, filters, embedding_model (a single one), strategy_mode , model, platform, query, system, llm_template
 * retrieve_sort_llm:index, query, top_k, filters, sort_desc, sort_type, model, platform, query, system, llm_template
 * retrieve_merge_llm: index, query, top_k, filters, model, platform, query, system, llm_template
 * retrieve_fulldocument:index, query, top_k, filters
 * retrieve_fulldoc_llm: index, query, top_k, filters, model, platform, query, system, llm_template
 * retrieve_batchmerge_llm: index, query, top_k, filters, model, platform, query, system, llm_template
 * expand_query_lang_llm: langs (list of languages to expand), index, query, model, platform, llm_template
 * dalle: style, size, quality, model, query


### Compose actions template

All the actions have the same structure. A Factory class to select what type to execute within the action, an abstract method class and then one class per action type with the logic to execute it.
Every sorting action has a boolean action param called “desc” to set if the result should be descendant or ascendant

1. **Retrieve**

    This is to retrieve indexed documents based on a query. This is the most important action, and it is the one that will define our entry. In most cases, there should always be a retrieval that will usually be the first step of the flow. Once the search results are obtained in the format defined by the data model: By defauld, 1 streamlist with several streamchunk segments, other actions can be applied to them. It is also possible to store the chunks in different streamlists within the streambatch.

    Example json template:

    ```json
    {
        "action": "retrieve",
        "action_params": {
            "params": {
                "generic": {
                    "index_conf": {
                        "index": "$index",
                        "query": "$query",
                        "task": "retrieve",
                        "top_k": 5,
                        "filters": $filters
                    },
                    "process_type": "ir_retrieve"
                }
            },
            "type": "get_chunks"
        }
    }
    ```

    Parameters of the 'retrieve' action:

   - **Index** (string): Index name where the document to retrieve is stored.

   - **Query** (string): User query to execute.

   - **Top_k** (int): Number of chunks to retrieve.

   - **Filters** (json/dict): Filters to apply while retrieving the document chunks.

    Example:

    ```json
    "filters": {
        "filename": "Compose_Manual.docx"
    }
    ```

   - **Type** (string): Retrieve type. The available types are:

      1. "get_chunks": Calls genai-inforetrieval to get the K number of chunks from the specified index.
      2. "get_document": Calls genai-inforetrieval to get the entire document content specified.

    With the same action in the template json file, we can execute multiple retrieval to obtain different streamlist with different queries using the "retrieve" action name in the json api call.

    ```json
    "retrieve": [
        {
            "query": "What are the New Year's resolutions?",
            "index": "myindex",
            "filters": {
                "filename": "myfile.txt"
            }
        },
        {
            "query": "What can I gift for Christmas?",
            "index": "myindex2",
            "filters": {
                "filename": "myfile2.docx"
            }
        }
    ]
    ```

2. **Filter**

    This action is executed at the streamlist level. The aim is to apply a filter to the streamlist (that is, to the list of retrieved documents). For example, you could apply a filter to keep only 5 chunks  out of the 10 chunks that the genai-inforetrieval returned. Another example would be to filter those documents that contain a certain metadata or that are not really related to the input query.

    Parameters of the action 'filter:

    - **Type** (string): Filter type to execute. (top_k, related_to, metadata, permissionfilter)

    - **Top_k** (int): Number of chunks to return.

    - **Model** (string): LLM model to use.

    - **Platform** (string): Platform hosting the LLM.

    - **Query** (string)

    - **Template_name** (string):  Template name to use while calling genai-llmapi.

    - **Filer_conditions** (json/dict): Conditions to check in the retrieved chunks to filter them. Using “or”, “and” to combine conditions and each condition is structured the same way, {“condition type”: {“metadata name”: “condition”}}

    Example of 'filter' action using type **"related_to"**:

    ```json
    {
        "action": "filter",
        "action_params":  {
            "type":  "related_to",
            "params":  {
                "llm_metadata":  {
                    "model":  "gpt-3.5-16k-pool-techhub-europe"
                },
                "platform_metadata":  {
                    "platform":  "azure"
                },
                "query_metadata":  {
                    "query":  "$query",
                    "template_name":  "query_and_context_related"
                }
            }
        }
    }
    ```

    Example of 'filter' action using type **"metadata"**:

    ```json
    {
        "type": "metadata",
        "params": {
            "filter_conditions":  {
                "or": [
                    {"eq": ("city", "New York")}, # Checks if a metadata is equal to a value.
                    {"in": ("city", ["New York", "London"])},  # Checks if a metadata is in a list of values.
                    {"textinmeta": ("city", "New Yor")}, # Checks if a string is contained in the metadata.
                    {"metaintext": ("city", "The city of New York is known as the big apple.")} 
                ],
                "and": [
                    {"gt": ("age", 30)}, # Checks if a metadata is greater than a value.
                    {"lt": ("age", 40)}  # Checks if a metadata is lower than a value.
                    {"gt_date": ["date", "2023-12-17"]}, #Greater than variation to work with date values
                    {"lt_date": ["date", "2023-12-19"]} #Lower than variation to work with date values
                ]
            },
            "operator": "and" # Operator to combine the previous group of filters. It can be 'and' or 'or'.
        }
    }
    ```

    |Allowed date types|
    | - |
    |Yyyy-mm-dd|
    |Yyyy/mm/dd|
    |Yyyy/mm|
    |yyyy|
    |Yyyymmdd|
    |Mmddyy|
    |Mmddyyyy|
    |Mm/dd/yy|
    |Mm/dd/yyyy|
    |Mm-dd-yy|
    |Mm-dd-yyyy|

3. **Merge**

   This action merges the different streamchunks in a streamlist into a single streamchunk. Starts with 1 streambatch containing 1 streamlist with multiple streamchunks and it ends with a streambatch containing 1 streamlist with the merged content in 1 chunk. It is also possible to set a grouping key to get the result in different streamchunks, 1 streamchunk per group.
   The result chunk will have the merged information in the content field and in the metadata common to all the chunks merged will be saved in the new chunk.


    - **Type** (string): Merge type to execute. (meta)

    - **Template** (string): Template to used to set the structure of the result content, the words starting with “$” represents the value of metadata or attribute of each chunk.

    - **Sep** (string): Value to use to separate each content chunk.

    - **Grouping_key** (string): Value to group the results.

    Example for action 'merge' in template:

    ```json
    {          
    "action":  "merge",
    "action_params": {
        "params":  {
            "template": "Content: $content, Date: $date, Doc name: $document_name",
            "sep": "####"
            },
        "type":  "meta"
        }
    }
    ```

4. **Batchmerge**

   This action merges the different streamlist in a streambatch into a single streamlist with non-repeated chunks. Starts with 1 streambatch containing multiple streamlist containing multiple chunks that can be repeated between streamlists and it ends with a streambatch containing 1 streamlist with unique chunks.

   Example for action 'batchmerge' in template:

   ```json
    {
        "action": "batchmerge",
        "action_params": {
            "params": {
            },
            "type": "add"
        }
    }
   ```

5. **Sort**

   Can be executed for the streambatch or for streamlist. It can sort the streamlist based on the score, content length, document id or snippet number and the streambatch based on the mean score or the overall content. It can also sort based on other specified metadata or date. The usable date formats are the same as for the 'filter' action.

   - **Type** (string): Sorting type to execute.
        - **Score**: Sorts by the mean score of each chunk.
        - **Length**: Sorts by the length of each chunk.
        - **Doc_id**: Sort by the document id of each chunk.
        - **Sn_number**: Sort by the snippet number of each chunk.
        - **Date**: Sort by metadata named “date” with date type values.
        - **Meta**: Sort by the metadata value, date values don’t work in this type.
   - **Desc** (bool): Sort descendant or ascendant.
   - **Value**: Metadata to use while sorting the streamlist.

   Example of action 'sort' with type **"length"**:

   ```json
   {
        "action": "sort",    
        "action_params": {
            "type":  "length",
            "params":  {
                "desc":  true
            }
        }
    }
   ```

   Example of action 'sort' with type **"meta"**:

   ```json
   {
        "action": "sort",    
        "action_params": {
            "type":  "meta",
            "params":  {
                "desc":  true,
                "value": $metadata_name
            }
        }
    }
   ```

6. **Groupby**

    This action sorts the streamlist by groups. Each group will be sorted by snippet_number, like its natural order and then the groups can be sorted by the maximum score from each group, the mean score from each group and by date.

   - **Type** (string): Groupby type to use. (docscore, date).

   - **Method** (string): Method to use in the docscore sorting (max, mean).

   - **Desc** (bool): Sort descendant or ascendant.

    Example for 'groupby' action with type "docscore":

    ```json
    {
        "action": "groupby",    
        "action_params": {
            "params":  {
                "desc":  true,
                "method": "max"
            },
            "type": "docscore"
        }
    }
    ```

    Example for 'groupby' action with type **"date"**:

    ```json
    {
        "action": "groupby",    
        "action_params": {
            "params":  {
                "desc":  true
            },
            "type": "date"
        }
    }
    ```

7. **LLM_action**

   This is the action that calls the LLM service (that is, it calls an LLM). This is the action where the LLM template (“template_name”) must be defined. Here we can also define the system and query that will be sent to the LLM.

   Parameters of this action:

   - **Type** (string): Method to user while calling genai-llmapi. (llm_content, llm_segments)

   - **Model** (string): LLM model to use.

   - **Platform** (string): Platform hosting the LLM.

   - **Query** (string)

   - **Template_name** (string):  Template name to use while calling genai-llmapi.

   - **System** (string): Context and task that will be sent to the LLM.

   Within this action, there are two types:

   - **Llm_content**: This gets all the document fragments retrieved and merges them into a single fragment that then is sent to the LLM. It returns a single response with a streambatch of one streamlist containing all the chunks retrieved and the and the last element of the streamlist will be the answer generated by the LLM.

    Example for 'llm_action' action with type "llm_content":

    ```json
        {   
            "action": "llm_action",
            "action_params": {
                "params": {
                    "llm_metadata": {
                        "model": "gpt-3.5-16k-pool-techhub-europe",
                        "max_input_tokens":5000
                    },
                    "platform_metadata": {
                        "platform": "azure"
                    },
                    "query_metadata": {
                        "query": "$query",
                        "system":"You are a helpful assistant",
                        "template_name": "system_query_and_context_plus"
                    }
                },
                "type": "llm_content"
            }
        }
    ```

   - **Llm_segments**: This takes each one of the document fragments and sends them individually to the LLM. Therefore, you will get as many responses as document fragments you sent. The response will contain a streambatch of one streamlist containing the chunks retrieved with each answer.


    Example for 'llm_action' action with type "llm_segment":

    ```json
    {   
        "action": "llm_action",
        "action_params": {
            "params": {
                "llm_metadata": {
                    "model": "gpt-3.5-16k-pool-techhub-europe",
                    "max_input_tokens":5000
                },
                "platform_metadata": {
                    "platform": "azure"
                },
                "query_metadata": {
                    "query": "$query",
                    "system":"You are a helpful assistant",
                    "template_name": "system_query_and_context_plus"
                }
            },
            "type": "llm_segment"
        }
    }
    ```

8. **Query expansion**
    This action allows the user to expand the original query in multiple queries in order to improve the LLM response or the chunks retrieved.

   Parameters of this action:

   - **Type** (string): Method to use for the expansion. (langs)

   - **Langs** (string, list): Languages to translate the query while using the langs type.

   Within this action, there is one type:

   - **Lang Expansion**: This expansion method, translates the original query to the received languages by calling genai-llmapi and creates new retrieve action steps in order to call genai-inforetrieval with each query. In languages list the user can specify the entire language or an abbreviation like "en" or "ja". Param model is optional.

    Example:

    ```json
   {
        "action": "expansion",
        "action_params":{
            "params": {
                "langs" : ["es", "ja", "chinese"],
                "model": "techhubinc-pool-us-gpt-3.5-turbo-16k"
            },
            "type": "lang"
        }
    }
    ```

    The available abbreviations are:
    - "ja": "japanese",
    - "es": "spanish",
    - "en": "english",
    - "fr": "french",
    - "de": "german",
    - "zh": "chinese",
    - "it": "italian",
    - "ko": "korean",
    - "pt": "portuguese",
    - "ru": "russian",
    - "ar": "arabic",
    - "hi": "hindi",
    - "tr": "turkish",
    - "nl": "dutch",
    - "sv": "swedish",
    - "pl": "polish",
    - "el": "greek",
    - "he": "hebrew",
    - "vi": "vietnamese",
    - "th": "thai",
    - "ca": "catalan"

9. **Reformulate query**
    This action allows the user to reformulate the original query to improve the quality of the responses.

   Parameters of this action:

   - **Type** (string): Method to use for the reformulate. (mix_queries)

   Within this action, there is one type:

   - **Mix_queries**: This type reformulates the query using the session context to make a better query for the LLM. For example, the first query is "What is the capital of Spain?" and the second query could be "How many people live there?". The reformulate method will change the second query to something like this: "How many people live in Madrid?".

    Example:

    ```json
   {
        "action": "reformulate_query",
        "action_params":{
            "params":{
                "max_persistence": 3,
                "template_name": "reformulate",
                "save_mod_query": false
            },
            "type": "mix_queries"
        }
    }
    ```

    - Params:
      - Max_persistence (int): Max number of older sessions to use.
      - Template_name (string): Template used while calling the LLMAPI.
      - Save_mod_query (bool): Used to save or not the original query.

10. **filter query**
    this action allows the user to filter the query to protect the tool from malicious queries or not wanted topics. If the query is filtered, compose will cancel all the following actions.

    parameters of this action:

    - **type** (string): method to use for the reformulate. (llm)

    within this action, there is one type:

    - **llm**: this type filters the query using the llmapi and a template with the different categories. the template must be stored in the folder src/compose/filter_templates.

    example action:

    ```json
    {
        "action": "filter_query",
        "action_params":{
            "params": {
                "template" : "query_filter"
            },
            "type": "llm"
        }
    }
    ```

    Parameters:

    - **template**: Filter template name to use.

    Example filter template:

    ```json
    {
        "filter_types": ["GPT"],
        "substitutions_template": "Classifies the query into one of the following categories:
            1) Drugs: when the query mentions drugs or drug dealers.
            2) Politics: If the query mentions politicians...
            3) Other: If the consultation does not mention any of the above topics.
        Answer only with the category and topic (do not give any explanation or reasoning). Query: ",
        "substitutions": [{
                "from": "Drugs",
                "to": "Always start the answer by saying that you are a very young AI and should not be asked about drugs and that you should be ashamed to ask such a question to someone so young, and that you prefer to talk about: ",
                "extra_words": ["climate", "nature", "landscapes", "flora and fauna"],
                "randpick": 3
            }, {
                "from": "Politics",
                "to": "Always start the answer by saying that you are an AI that has just been created and that there are many topics in politics that you are still learning about and you prefer not to give your opinion without knowing, and that you prefer to chat about: ",
                "extra_words": ["climate", "nature", "landscapes", "flora and fauna"],
                "randpick": 3
            }
    ]}
    ```

    Parameters:

    - **filter_types**: Currently There is only one type of filter, GPT.

    - **substitutions_template**: It will be the prompt used for classification.

    - **substitutions**: It will be defined in the format "from to" and will specify the type of substitution. Each type is defined differently.

    - **GPT**: The "from" should define the type, the "to" should specify the GPT substitution prompt, and optionally, a list of elements can be added through "extra_words" (which defines the vocabulary) and "randpick" (which randomly selects the number of words to include to make the GPT response unique).

11. **Filter response**
    This action allows the user to filter the response to double check if the awnswer is correct or if the topic from the answer is not desired.

    Parameters of this action:

    - **Type** (string): Method to use for the reformulate. (llm)

    Within this action, there is one type:

    - **LLM**: This type filters the response using the LLMAPI and a template with the different categories. The template must be stored in the folder src/compose/filter_templates.

    Example action:

    ```json
        {
        "action": "filter_response",
        "action_params":{
            "params": {
                "template" : "response_filter"
            },
            "type": "llm"
        }
    }
    ```

    Example filter template:

    ```json
    {
        "filter_types": [
            "GPT"
        ],
        "substitutions_template": "Classify the 'Response' into one of the following categories: \n1) Correct: When the 'Response' is related to the 'query'. \n2) Incorrect: The 'Response' is not related to the 'query'. \n3) Sensitive Information: The 'Response' contains sensitive information such as ID numbers, customer numbers, usernames, etc.",
        "substitutions": [{
            "from": "Correct",
            "to": null
            },
            {
                "from": "Incorrect",
                "to": "Notify that a hallucination has been detected in the generated response and that the query cannot be answered.",
                "extra_words": [
                    "weather",
                    "nature"
                ],
                "randpick": 3
            }, {
                "from": "Sensitive Information",
                "to": "Notify that sensitive information has been detected and that the query cannot be answered. Suggest discussing:",
                "extra_words": [
                    "weather",
                    "nature",
                    "landscapes",
                    "flora and fauna",
                    "geography"
                ],
                "randpick": 3
            }
        ]
    }
    ```

    Parameters:

    - **filter_types**: Currently There is only one type of filter, GPT.

    - **substitutions_template**: It will be the prompt used for classification.

    - **substitutions**: It will be defined in the format "from to" and will specify the type of substitution. Each type is defined differently.

    - **GPT**: The "from" should define the type, the "to" should specify the GPT substitution prompt, and optionally, a list of elements can be added through "extra_words" (which defines the vocabulary) and "randpick" (which randomly selects the number of words to include to make the GPT response unique).

### LLM Prompt Templates

LLM templates are used to configure the prompts that are sent to the generative language models. When the LLMAPI subcomponent is initialized, it reads all the files in the directory and loads to memory all the templates, removing duplicates. Below is an example of what an LLM templates file looks, with each key in the JSON representing a different template:

```json
{
    "rag_with_references": {
        "system": "You are an AI assistant that perform the requested task in the 'UserTask' using the 'TaskContext'. Ignore any previous questions, focus only on resolve 'UserTask' with the text provided in 'TaskContext'.",
        "user": "Answer the task in 'UserTask' with the information provided in 'TaskContext'. \n Add to the answer the citation to the 'Filename' or 'Filenames' that corresponds to the text used to answer the task. Use the following format for your citations output: (Source: Filename).\n\n Important:\n 1. Ignore any previous questions, focus only on resolve 'UserTask' with the text provided in 'TaskContext'. \n 2. Answer the 'UserTask' if the information is in the context 'TaskContext' or if 'UserTask' contains a generic task like summarize, extract concepts or topics, otherwise answer 'not found' (In this case you do not add the citation to the filename in the answer). \n\n You must not mention TaskContext or UserTask in the answer or the citations.\n\n \n\n########################\n\n 'UserTask': '$query' \n\n########################\n\n 'TaskContext':\n\n $context \n\n########################\n\n Answer in English:"
    },
    "system_query": {
        "system": "$system",
        "user": "$query"
    },
	"system_query_v": {
		"system": "$system",
		"user": ["$query"]
	},
    "system_query_and_context_plus": {
        "system": "You are an assistant. You must answer questions in English (translating the context to English if neccesary), regardless of the input language or whether another language is indicated in the question.",
        "user": "Answer the following task based on the following 'context' or the history of the conversation. if the information is not in the context provided, responds 'I can not answer your question'. \nTask: '$query' \n\n######\n\nContext:\n$context \n\n######\n\nAnswer:"
    },
}
```

### Models embeddings

The indexation pipeline uses various embedding models across different platforms (OpenAI's ADA model) deployed in various geographical regions, as well as different pools* of models to allow a more balanced deployment of models. It is important to know that a compatible model must be used in the retrieval process for the system to work correctly. The list of available models, along with the pool they belong to, is below.
The available models depend on the region where the suscription is deployed. Make sure the model is available in the region you are using.

- Japan region

| Model Name            | Pools                                                  | Platform |
|-----------------------|--------------------------------------------------------|------|
| ada-002-pro-japaneast | ada-002-pool-techhub-japan, ada-002-pool-techhub-world |azure|
| ada-002-inc-japaneast | ada-002-pool-techhub-japan, ada-002-pool-techhub-world |azure|

- Inc region

| Model Name | Pools | Platform |
|--------|------|------|
|techhubinc-ada-002-australiaeast|techhub-pool-world-ada-002|azure|
|techhubinc-ada-002-brazilsouth|techhub-pool-world-ada-002|azure|
|techhubinc-ada-3-large-canadaeast|techhub-pool-world-ada-3-large|azure|
|techhubinc-ada-3-small-canadaeast|techhub-pool-world-ada-3-small|azure|
|techhubinc-ada-002-eastus|techhub-pool-us-ada-002, techhub-pool-world-ada-002|azure|
|techhubinc-ada-3-large-eastus|techhub-pool-us-ada-3-large, techhub-pool-world-ada-3-large|azure|
|techhubinc-ada-3-small-eastus|techhub-pool-us-ada-3-small, techhub-pool-world-ada-3-small|azure|
|techhubinc-ada-002-eastus2|techhub-pool-us-ada-002, techhub-pool-world-ada-002|azure|
|techhubinc-ada-3-large-eastus2|techhub-pool-us-ada-3-large, techhub-pool-world-ada-3-large|azure|
|techhubinc-ada-3-small-eastus2|techhub-pool-us-ada-3-small, techhub-pool-world-ada-3-small|azure|
|techhubinc-ada-002-francecentral|techhub-pool-eu-ada-002, techhub-pool-world-ada-002|azure|
|techhubinc-ada-3-large-francecentral|techhub-pool-eu-ada-3-large, techhub-pool-world-ada-3-large|azure|
|techhubinc-ada-002-japaneast|techhub-pool-world-ada-002|azure|
|techhubinc-ada-3-large-japaneast|techhub-pool-world-ada-3-large|azure|
|techhubinc-ada-002-northcentralus|techhub-pool-us-ada-002, techhub-pool-world-ada-002|azure|
|techhubinc-ada-002-norwayeast|techhub-pool-world-ada-002|azure|
|techhubinc-ada-3-large-norwayeast|techhub-pool-world-ada-3-large|azure|
|techhubinc-ada-002-southafricanorth|techhub-pool-world-ada-002|azure|
|techhubinc-ada-002-southcentralus|techhub-pool-us-ada-002, techhub-pool-world-ada-002|azure|
|techhubinc-ada-002-southindia|techhub-pool-world-ada-002|azure|
|techhubinc-ada-002-swedencentral|techhub-pool-eu-ada-002, techhub-pool-world-ada-002|azure|
|techhubinc-ada-3-large-swedencentral|techhub-pool-world-ada-3-large|azure|
|techhubinc-ada-002-switzerlandnorth|techhub-pool-world-ada-002|azure|
|techhubinc-ada-002-uksouth|techhub-pool-world-ada-002|azure|
|techhubinc-ada-3-large-uksouth|techhub-pool-world-ada-3-large|azure|
|techhubinc-ada-002-westeurope|techhub-pool-eu-ada-002, techhub-pool-world-ada-002|azure|
|techhubinc-ada-002-westus|techhub-pool-us-ada-002, techhub-pool-world-ada-002|azure|
|techhubinc-ada-002-westus3|techhub-pool-us-ada-002, techhub-pool-world-ada-002|azure|
|techhubinc-ada-3-large-westus3|techhub-pool-us-ada-3-large, techhub-pool-world-ada-3-large|azure|
|dpr-encoder|No pools (huggingface models are downloaded)|huggingface|

- Dev region
| Model Name | Pools | Platform |
|--------|------|------|
|techhubdev-ada-002-australiaeast|techhub-pool-world-ada-002|azure|
|techhubdev-ada-002-brazilsouth|techhub-pool-world-ada-002|azure|
|techhubdev-ada-002-canadaeast|techhub-pool-world-ada-002|azure|
|techhubdev-ada-3-large-canadaeast|techhub-pool-world-ada-3-large|azure|
|techhubdev-ada-3-small-canadaeast|techhub-pool-world-ada-3-small|azure|
|techhubdev-ada-002-eastus|techhub-pool-us-ada-002, techhub-pool-world-ada-002|azure|
|techhubdev-ada-3-large-eastus|techhub-pool-us-ada-3-large, techhub-pool-world-ada-3-large|azure|
|techhubdev-ada-3-small-eastus|techhub-pool-us-ada-3-small, techhub-pool-world-ada-3-small|azure|
|techhubdev-ada-002-eastus2|techhub-pool-us-ada-002, techhub-pool-world-ada-002|azure|
|techhubdev-ada-3-large-eastus2|techhub-pool-us-ada-3-large, techhub-pool-world-ada-3-large|azure|
|techhubdev-ada-3-small-eastus2|techhub-pool-us-ada-3-small, techhub-pool-world-ada-3-small|azure|
|techhubdev-ada-002-francecentral|techhub-pool-eu-ada-002, techhub-pool-world-ada-002|azure|
|techhubdev-ada-3-large-francecentral|techhub-pool-eu-ada-3-large, techhub-pool-world-ada-3-large|azure|
|techhubdev-ada-002-japaneast|techhub-pool-world-ada-002|azure|
|techhubdev-ada-3-large-japaneast|techhub-pool-world-ada-3-large|azure|
|techhubdev-ada-3-small-japaneast|techhub-pool-world-ada-3-small|azure|
|techhubdev-ada-002-norwayeast|techhub-pool-world-ada-002|azure|
|techhubdev-ada-3-large-norwayeast|techhub-pool-world-ada-3-large|azure|
|techhubdev-ada-002-southafricanorth|techhub-pool-world-ada-002|azure|
|techhubdev-ada-002-southcentralus|techhub-pool-us-ada-002, techhub-pool-world-ada-002|azure|
|techhubdev-ada-002-southindia|techhub-pool-world-ada-002|azure|
|techhubdev-ada-3-large-southindia|techhub-pool-world-ada-3-large|azure|
|techhubdev-ada-002-swedencentral|techhub-pool-eu-ada-002, techhub-pool-world-ada-002|azure|
|techhubdev-ada-3-large-swedencentral|techhub-pool-world-ada-3-large|azure|
|techhubdev-ada-002-switzerlandnorth|techhub-pool-world-ada-002|azure|
|techhubdev-ada-002-uksouth|techhub-pool-world-ada-002|azure|
|techhubdev-ada-3-large-uksouth|techhub-pool-world-ada-3-large|azure|
|techhubdev-ada-002-westeurope|techhub-pool-eu-ada-002, techhub-pool-world-ada-002|azure|
|techhubdev-ada-002-westus|techhub-pool-us-ada-002, techhub-pool-world-ada-002|azure|
|techhubdev-ada-002-westus3|techhub-pool-us-ada-002, techhub-pool-world-ada-002|azure|
|techhubdev-ada-3-large-westus3|techhub-pool-us-ada-3-large, techhub-pool-world-ada-3-large|azure|
|dpr-encoder|No pools (huggingface models are downloaded)|huggingface|
|dunzhang-stella-1.5B-v5|No pools (huggingface models are downloaded)|huggingface|

*A pool of models is a group of the same models allocated in different servers from a specific region, such as Europe or the US, that allows a more balanced deployment of models.*

### Models LLM

There is a variety of generative language models available for different use cases within the retrieval pipeline. Below is the list of these models, including an example of a pool to which each below. A pool of models is a group of the same models allocated in different servers from a specific region, such as Europe or the US, that allows a more balanced deployement of models.
The available models depend on the region where the suscription is deployed. Make sure the model is available in the region you are using.

- Japan region

| Model Name                    | Pools                                               | Platform |
|-------------------------------|-----------------------------------------------------|------|
|gpt-4o-judge|gpt-4o-pool-techhub-japan, gpt-4o-pool-techhub-world|azure|
|techhubinc-JapanEast-gpt-4o-2024-05-13|gpt-4o-pool-techhub-japan, gpt-4o-pool-techhub-world|azure|
|gpt-35-turbo-16k|gpt-3.5-16k-pool-techhub-japan, gpt-3.5-16k-pool-techhub-world|azure|
|techhubinc-JapanEast-gpt-35-turbo-16k-0613|gpt-3.5-16k-pool-techhub-japan, gpt-3.5-16k-pool-techhub-world|azure|


- Inc region

| Model Name | Pools | Platform |
|--------|------|------|
|techhubinc-AustraliaEast-dall-e-3|techhubinc-pool-world-dalle3|azure|
|techhubinc-AustraliaEast-gpt-35-turbo-16k-0613|techhubinc-pool-world-gpt-3.5-turbo-16k|azure|
|techhubinc-AustraliaEast-gpt-4-turbo-2024-04-09|techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-AustraliaEast-gpt-4o-2024-05-13|techhubinc-pool-world-gpt-4o|azure|
|techhubinc-AustraliaEast-gpt-4-vision-preview|techhubinc-pool-world-gpt-4v|azure|
|techhubinc-BrazilSouth-gpt-4-turbo-2024-04-09|techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-BrazilSouth-gpt-4o-2024-05-13|techhubinc-pool-world-gpt-4o|azure|
|techhubinc-CanadaEast-gpt-35-turbo-16k-0613|techhubinc-pool-world-gpt-3.5-turbo-16k|azure|
|techhubinc-CanadaEast-gpt-4-turbo-2024-04-09|techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-CanadaEast-gpt-4o-2024-05-13|techhubinc-pool-world-gpt-4o|azure|
|techhubinc-EastUS-dall-e-3|techhubinc-pool-us-dalle3, techhubinc-pool-world-dalle3|azure|
|techhubinc-EastUS-gpt-35-turbo-16k-0613|techhubinc-pool-us-gpt-3.5-turbo-16k, techhubinc-pool-world-gpt-3.5-turbo-16k|azure|
|techhubinc-EastUS-gpt-4-turbo-2024-04-09|techhubinc-pool-us-gpt-4-turbo, techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-EastUS-gpt-4o-2024-05-13|techhubinc-pool-us-gpt-4o, techhubinc-pool-world-gpt-4o|azure|
|techhubinc-EastUS-gpt-4o-mini-2024-07-18|techhubinc-pool-us-gpt-4o-mini, techhubinc-pool-world-gpt-4o-mini|azure|
|techhubinc-EastUS2-gpt-35-turbo-16k-0613|techhubinc-pool-us-gpt-3.5-turbo-16k, techhubinc-pool-world-gpt-3.5-turbo-16k|azure|
|techhubinc-EastUS2-gpt-4-turbo-2024-04-09|techhubinc-pool-us-gpt-4-turbo, techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-EastUS2-gpt-4o-2024-05-13|techhubinc-pool-us-gpt-4o, techhubinc-pool-world-gpt-4o|azure|
|techhubinc-EastUS2-gpt-4o-2024-08-06|techhubinc-pool-us-gpt-4o, techhubinc-pool-world-gpt-4o|azure|
|techhubinc-FranceCentral-gpt-35-turbo-16k-0613|techhubinc-pool-eu-gpt-3.5-turbo-16k, techhubinc-pool-world-gpt-3.5-turbo-16k|azure|
|techhubinc-FranceCentral-gpt-4-turbo-2024-04-09|techhubinc-pool-eu-gpt-4-turbo, techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-FranceCentral-gpt-4o-2024-05-13|techhubinc-pool-eu-gpt-4o, techhubinc-pool-world-gpt-4o|azure|
|techhubinc-GermanyWestCentral-gpt-4-turbo-2024-04-09|techhubinc-pool-eu-gpt-4-turbo, techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-GermanyWestCentral-gpt-4o-2024-05-13|techhubinc-pool-eu-gpt-4o, techhubinc-pool-world-gpt-4o|azure|
|techhubinc-JapanEast-gpt-4o-2024-05-13|techhubinc-pool-world-gpt-4o|azure|
|techhubinc-JapanEast-gpt-35-turbo-16k-0613|techhubinc-pool-world-gpt-3.5-turbo-16k|azure|
|techhubinc-JapanEast-gpt-4-turbo-2024-04-09|techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-JapanEast-gpt-4-vision-preview|techhubinc-pool-world-gpt-4v|azure|
|techhubinc-KoreaCentral-gpt-4-turbo-2024-04-09|techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-KoreaCentral-gpt-4o-2024-05-13|techhubinc-pool-world-gpt-4o|azure|
|techhubinc-NorthCentralUS-gpt-4o-2024-08-06|techhubinc-pool-us-gpt-4o, techhubinc-pool-world-gpt-4o|azure|
|techhubinc-NorthCentralUS-gpt-35-turbo-16k-0613|techhubinc-pool-us-gpt-3.5-turbo-16k, techhubinc-pool-world-gpt-3.5-turbo-16k|azure|
|techhubinc-NorthCentralUS-gpt-4-turbo-2024-04-09|techhubinc-pool-us-gpt-4-turbo, techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-NorwayEast-gpt-4-turbo-2024-04-09|techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-NorwayEast-gpt-4o-2024-05-13|techhubinc-pool-world-gpt-4o|azure|
|techhubinc-PolandCentral-gpt-4o-2024-05-13|techhubinc-pool-eu-gpt-4o, techhubinc-pool-world-gpt-4o|azure|
|techhubinc-PolandCentral-gpt-4-turbo-2024-04-09|techhubinc-pool-eu-gpt-4-turbo, techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-SouthAfricaNorth-gpt-4-turbo-2024-04-09|techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-SouthAfricaNorth-gpt-4o-2024-05-13|techhubinc-pool-world-gpt-4o|azure|
|techhubinc-SouthCentralUS-gpt-4-turbo-2024-04-09|techhubinc-pool-us-gpt-4-turbo, techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-SouthCentralUS-gpt-4o-2024-08-06|techhubinc-pool-us-gpt-4o, techhubinc-pool-world-gpt-4o|azure|
|techhubinc-SouthIndia-gpt-4o-2024-05-13|techhubinc-pool-world-gpt-4o|azure|
|techhubinc-SouthIndia-gpt-4-turbo-2024-04-09|techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-SwedenCentral-gpt-35-turbo-16k-0613|techhubinc-pool-eu-gpt-3.5-turbo-16k, techhubinc-pool-world-gpt-3.5-turbo-16k|azure|
|techhubinc-SwedenCentral-gpt-4-turbo-2024-04-09|techhubinc-pool-eu-gpt-4-turbo, techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-SwedenCentral-gpt-4-vision-preview|techhubinc-pool-eu-gpt-4v, techhubinc-pool-world-gpt-4v|azure|
|techhubinc-SwedenCentral-gpt-4o-2024-08-06|techhubinc-pool-eu-gpt-4o, techhubinc-pool-world-gpt-4o|azure|
|techhubinc-SwedenCentral-gpt-4o-mini-2024-07-18|techhubinc-pool-eu-gpt-4o-mini, techhubinc-pool-world-gpt-4o-mini|azure|
|techhubinc-SwitzerlandNorth-gpt-35-turbo-16k-0613|techhubinc-pool-world-gpt-3.5-turbo-16k|azure|
|techhubinc-SwitzerlandNorth-gpt-4-turbo-2024-04-09|techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-SwitzerlandNorth-gpt-4-vision-preview|techhubinc-pool-world-gpt-4v|azure|
|techhubinc-SwitzerlandNorth-gpt-4o-2024-05-13|techhubinc-pool-world-gpt-4o|azure|
|techhubinc-UKSouth-gpt-4o-2024-05-13|techhubinc-pool-world-gpt-4o|azure|
|techhubinc-UKSouth-gpt-35-turbo-16k-0613|techhubinc-pool-world-gpt-3.5-turbo-16k|azure|
|techhubinc-UKSouth-gpt-4-turbo-2024-04-09|techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-WestEurope-gpt-4-turbo-2024-04-09|techhubinc-pool-eu-gpt-4-turbo, techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-WestEurope-gpt-4o-2024-05-13|techhubinc-pool-eu-gpt-4o, techhubinc-pool-world-gpt-4o|azure|
|techhubinc-WestUS-gpt-4o-2024-08-06|techhubinc-pool-us-gpt-4o, techhubinc-pool-world-gpt-4o|azure|
|techhubinc-WestUS-gpt-4-turbo-2024-04-09|techhubinc-pool-us-gpt-4-turbo, techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-WestUS-gpt-4-vision-preview|techhubinc-pool-us-gpt-4v, techhubinc-pool-world-gpt-4v|azure|
|techhubinc-WestUS3-gpt-4-turbo-2024-04-09|techhubinc-pool-us-gpt-4-turbo, techhubinc-pool-world-gpt-4-turbo|azure|
|techhubinc-WestUS3-gpt-4o-2024-08-06|techhubinc-pool-us-gpt-4o, techhubinc-pool-world-gpt-4o|azure|

- Dev region
| Model Name | Pools | Platform |
|--------|------|------|
|techhubdev-AustraliaEast-dall-e-3|techhubdev-pool-world-dalle3|azure|
|techhubdev-AustraliaEast-gpt-35-turbo-16k-0613|techhubdev-pool-world-gpt-3.5-turbo-16k|azure|
|techhubdev-AustraliaEast-gpt-4-turbo-2024-04-09|techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-AustraliaEast-gpt-4o-2024-05-13|techhubdev-pool-world-gpt-4o|azure|
|techhubdev-AustraliaEast-gpt-4-vision-preview|techhubdev-pool-world-gpt-4v|azure|
|techhubdev-BrazilSouth-gpt-4-turbo-2024-04-09|techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-BrazilSouth-gpt-4o-2024-05-13|techhubdev-pool-world-gpt-4o|azure|
|techhubdev-CanadaEast-gpt-35-turbo-16k-0613|techhubdev-pool-world-gpt-3.5-turbo-16k|azure|
|techhubdev-CanadaEast-gpt-4-turbo-2024-04-09|techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-CanadaEast-gpt-4o-2024-05-13|techhubdev-pool-world-gpt-4o|azure|
|techhubdev-EastUS-dall-e-3|techhubdev-pool-us-dalle3, techhubdev-pool-world-dalle3|azure|
|techhubdev-EastUS-gpt-35-turbo-16k-0613|techhubdev-pool-us-gpt-3.5-turbo-16k, techhubdev-pool-world-gpt-3.5-turbo-16k|azure|
|techhubdev-EastUS-gpt-4-turbo-2024-04-09|techhubdev-pool-us-gpt-4-turbo, techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-EastUS-gpt-4o-2024-05-13|techhubdev-pool-us-gpt-4o, techhubdev-pool-world-gpt-4o|azure|
|techhubdev-EastUS-gpt-4o-mini-2024-07-18|techhubdev-pool-us-gpt-4o-mini, techhubdev-pool-world-gpt-4o-mini|azure|
|techhubdev-EastUS2-gpt-35-turbo-16k-0613|techhubdev-pool-us-gpt-3.5-turbo-16k, techhubdev-pool-world-gpt-3.5-turbo-16k|azure|
|techhubdev-EastUS2-gpt-4-turbo-2024-04-09|techhubdev-pool-us-gpt-4-turbo, techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-EastUS2-gpt-4o-2024-08-06|techhubdev-pool-us-gpt-4o, techhubdev-pool-world-gpt-4o|azure|
|techhubdev-FranceCentral-gpt-35-turbo-16k-0613|techhubdev-pool-eu-gpt-3.5-turbo-16k, techhubdev-pool-world-gpt-3.5-turbo-16k|azure|
|techhubdev-FranceCentral-gpt-4-turbo-2024-04-09|techhubdev-pool-eu-gpt-4-turbo, techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-FranceCentral-gpt-4o-2024-05-13|techhubdev-pool-eu-gpt-4o, techhubdev-pool-world-gpt-4o|azure|
|techhubdev-GermanyWestCentral-gpt-4-turbo-2024-04-09|techhubdev-pool-eu-gpt-4-turbo, techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-GermanyWestCentral-gpt-4o-2024-05-13|techhubdev-pool-eu-gpt-4o, techhubdev-pool-world-gpt-4o|azure|
|techhubdev-JapanEast-gpt-4o-2024-05-13|techhubdev-pool-world-gpt-4o|azure|
|techhubdev-JapanEast-gpt-35-turbo-16k-0613|techhubdev-pool-world-gpt-3.5-turbo-16k|azure|
|techhubdev-JapanEast-gpt-4-turbo-2024-04-09|techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-JapanEast-gpt-4-vision-preview|techhubdev-pool-world-gpt-4v|azure|
|techhubdev-KoreaCentral-gpt-4-turbo-2024-04-09|techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-KoreaCentral-gpt-4o-2024-05-13|techhubdev-pool-world-gpt-4o|azure|
|techhubdev-NorthCentralUS-gpt-35-turbo-16k-0613|techhubdev-pool-us-gpt-3.5-turbo-16k, techhubdev-pool-world-gpt-3.5-turbo-16k|azure|
|techhubdev-NorthCentralUS-gpt-4-turbo-2024-04-09|techhubdev-pool-us-gpt-4-turbo, techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-NorwayEast-gpt-4-turbo-2024-04-09|techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-NorwayEast-gpt-4o-2024-05-13|techhubdev-pool-world-gpt-4o|azure|
|techhubdev-PolandCentral-gpt-4o-2024-05-13|techhubdev-pool-eu-gpt-4o, techhubdev-pool-world-gpt-4o|azure|
|techhubdev-PolandCentral-gpt-4-turbo-2024-04-09|techhubdev-pool-eu-gpt-4-turbo, techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-SouthAfricaNorth-gpt-4-turbo-2024-04-09|techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-SouthAfricaNorth-gpt-4o-2024-05-13|techhubdev-pool-world-gpt-4o|azure|
|techhubdev-SouthCentralUS-gpt-4-turbo-2024-04-09|techhubdev-pool-us-gpt-4-turbo, techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-SouthCentralUS-gpt-4o-2024-08-06|techhubdev-pool-us-gpt-4o, techhubdev-pool-world-gpt-4o|azure|
|techhubdev-SouthIndia-gpt-4o-2024-05-13|techhubdev-pool-world-gpt-4o|azure|
|techhubdev-SouthIndia-gpt-4-turbo-2024-04-09|techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-SwedenCentral-gpt-35-turbo-16k-0613|techhubdev-pool-eu-gpt-3.5-turbo-16k, techhubdev-pool-world-gpt-3.5-turbo-16k|azure|
|techhubdev-SwedenCentral-gpt-4-turbo-2024-04-09|techhubdev-pool-eu-gpt-4-turbo, techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-SwedenCentral-gpt-4-vision-preview|techhubdev-pool-eu-gpt-4v, techhubdev-pool-world-gpt-4v|azure|
|techhubdev-SwedenCentral-gpt-4o-2024-08-06|techhubdev-pool-eu-gpt-4o, techhubdev-pool-world-gpt-4o|azure|
|techhubdev-SwedenCentral-gpt-4o-mini-2024-07-18|techhubdev-pool-eu-gpt-4o-mini, techhubdev-pool-world-gpt-4o-mini|azure|
|techhubdev-SwitzerlandNorth-gpt-35-turbo-16k-0613|techhubdev-pool-world-gpt-3.5-turbo-16k|azure|
|techhubdev-SwitzerlandNorth-gpt-4-turbo-2024-04-09|techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-SwitzerlandNorth-gpt-4-vision-preview|techhubdev-pool-world-gpt-4v|azure|
|techhubdev-SwitzerlandNorth-gpt-4o-2024-05-13|techhubdev-pool-world-gpt-4o|azure|
|techhubdev-UKSouth-gpt-4o-2024-05-13|techhubdev-pool-world-gpt-4o|azure|
|techhubdev-UKSouth-gpt-35-turbo-16k-0613|techhubdev-pool-world-gpt-3.5-turbo-16k|azure|
|techhubdev-UKSouth-gpt-4-turbo-2024-04-09|techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-WestEurope-gpt-4-turbo-2024-04-09|techhubdev-pool-eu-gpt-4-turbo, techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-WestEurope-gpt-4o-2024-05-13|techhubdev-pool-eu-gpt-4o, techhubdev-pool-world-gpt-4o|azure|
|techhubdev-WestUS-gpt-4o-2024-08-06|techhubdev-pool-us-gpt-4o, techhubdev-pool-world-gpt-4o|azure|
|techhubdev-WestUS-gpt-4-turbo-2024-04-09|techhubdev-pool-us-gpt-4-turbo, techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-WestUS-gpt-4-vision-preview|techhubdev-pool-us-gpt-4v, techhubdev-pool-world-gpt-4v|azure|
|techhubdev-WestUS3-gpt-4-turbo-2024-04-09|techhubdev-pool-us-gpt-4-turbo, techhubdev-pool-world-gpt-4-turbo|azure|
|techhubdev-WestUS3-gpt-4o-2024-08-06|techhubdev-pool-us-gpt-4o, techhubdev-pool-world-gpt-4o|azure|

*A pool of models is a group of the same models allocated in different servers from a specific region, such as Europe or the US, that allows a more balanced deployment of models.*

## Examples

### Indexing Examples

Example of a request to the Global RAG indexing service:

~~~
import requests
import json
import base64

url =  "https://<deploymentdomain>/integrationasync/process"

############### PDF content ############### 
# The Bank of England paved the way for a summer interest rate cut yesterday after inflation fell to its 2 per cent target.
# Members of the Monetary Policy Committee (MPC) voted by seven to two to leave interest rates at 5.25 per cent.
# But the Bank revealed that, though it chose to hold the base rate at a 16-year high, it was a knife- edge decision.
# The announcement will revive hopes that the Bank could vote for a cut at the start of August, relieving pressure on millions of borrowers.
# It is expected to give a new Labour government an immediate boost should it win the General Election on July 4.

def read_pdf_to_base64(file_path):
    with open(file_path, 'rb') as file:
        content = file.read()

    base64_content = base64.b64encode(content).decode('utf-8')
    return base64_content

filename = "example_banking.pdf"
base64_pdf_content = read_pdf_to_base64(filename)

payload = {
  "index": "tech_hub_test",
  "operation": "indexing",
  "models": "techhubinc-ada-002-eastus2",
  "documents_metadata": {filename: {"content_binary": base64_pdf_content}},
  "window_length": 300,
  "window_overlap": 20,
  "metadata": {"year": "2024", "category": "banking"},
  "response_url": "http://"
}

headers = {
  "Content-type": "application/json",
  "x-api-key": "### ADD HERE API KEY"
}

response = requests.request("POST", url, headers=headers, json=payload)
~~~

Response from the indexing service:

~~~
{
  "status": "processing",
  "request_id": "request_20240627_134044_348410"
}
~~~

Once the process completes, the accessible endpoint provided by the user as callbak in the "response_url" parameter will receive a POST request with the following body:

~~~
{
  "status": "Finished",
  "request_id": "request_20240627_104218_291084",
  "index": "tech_hub_test",
  "docs": "filename.pdf"
}
~~~

### Compose Examples

Example of a request to the Global RAG compose service:

~~~
import requests
import json

url = "https://<deploymentdomain>/compose/process"

query = "Why did the Bank of England consider cutting interest rates after inflation fell to its 2 per cent target?"

index = "tech_hub_test"
top_k = 5
platform = "azure"
model = "gpt-3.5-16k-pool-techhub-japan"
system = "You are an assistant"
template_llm = "rag_with_references"

payload = {
  "generic": {
    "compose_conf": {
      "session_id": "651d79fa-9562-42d4-abc4-9e9addb17489",
      "template": {
        "name": "techhub_retrieval_reference",
        "params": {
          "index": index,
          "query": query,
          "top_k": top_k,
          "platform": platform,
          "system": system,
          "model": model,   
          "filters": {},
          "template_llm": template_llm      
        }
      }
    }
  }
}

headers = {
  "Content-type": "application/json",
  "x-api-key": "### ADD HERE API KEY"
}

response = requests.request("POST", url, headers=headers, data=payload)
~~~

Response from the compose service:

~~~
{
   "status":"finished",
   "result":{
      "session_id":"651d79fa-9562-42d4-abc4-9e9addb17489",
      "streambatch":[
         [
            {
               "content":"The Bank of England paved the way for a summer interest rate cut yesterday after inflation fell to its 2 per\ncent target.\n\nMembers of the Monetary Policy Committee (MPC) voted by seven to two to leave interest rates at 5.25 per\ncent.\n\nBut the Bank revealed that, though it chose to hold the base rate at a 16-year high, it was a knife- edge\ndecision.\n\nThe announcement will revive hopes that the Bank could vote for a cut at the start of August, relieving\npressure on millions of borrowers.\n\nIt is expected to give a new Labour government an immediate boost should it win the General Election on\nJuly 4.",
               "meta":{
                  "year":"2024",
                  "category":"banking",
                  "document_id":"078928e7-c0d5-40f9-acf5-06ca5f8ff442",
                  "uri":"*************************************************",
                  "sections_headers":"",
                  "tables":"",
                  "filename":"example_banking.pdf",
                  "snippet_number":0,
                  "snippet_id":"c3a74cdb-81ec-4282-8507-d94db3aeedde"
               },
               "scores":{
                  "bm25--None--score":0.9987273784672194,
                  "openai--text-embedding-ada-002--score":0.9458300000000008
               },
               "answer":"None"
            },
            {
               "content":"",
               "meta":{
                  "title":"Summary"
               },
               "scores":1,
               "answer":"The Bank of England considered cutting interest rates after inflation fell to its 2 per cent target in order to relieve pressure on borrowers and potentially boost the new Labour government if it won the General Election.",
               "tokens": {
                    "input_tokens": 1448,
                    "output_tokens": 7
               }
            }
         ]
      ],
      "answer":"The Bank of England considered cutting interest rates after inflation fell to its 2 per cent target in order to relieve pressure on borrowers and potentially boost the new Labour government if it won the General Election."
   },
   "status_code":200
}
~~~


* Simple retrieval

~~~
import requests
import json

url = "https://<deploymentdomain>/compose/process"

query = "Why did the Bank of England consider cutting interest rates after inflation fell to its 2 per cent target?"

index = "tech_hub_test"
top_k = 5

payload = {
  "generic": {
    "compose_conf": {
      "session_id": "651d79fa-9562-42d4-abc4-9e9addb17489",
      "template": {
        "name": "techhub_retrieval",
        "params": {
          "index": index,
          "query": query,
          "top_k": top_k,
          "filters": {}                 
        }
      }
    }
  }
}

headers = {
  "Content-type": "application/json",
  "x-api-key": "### ADD HERE API KEY"
}

response = requests.request("POST", url, headers=headers, data=payload)
~~~


* Multiple retrieval

If we want to retrieve information from multiple queries we can do a multiple retrieval. The request will include a key 'retrieve' as a list containing a dictionary for each query we want to retrieve information from our index.

~~~
import requests
import json

url = "https://<deploymentdomain>/compose/process"

query_1 = "inflation fell"
query_2 = "Monetary Policy Committee"

index = "tech_hub_test"
top_k = 5

payload = {
  "generic": {
    "compose_conf": {
      "session_id": "651d79fa-9562-42d4-abc4-9e9addb17489",
      "template": {
        "name": "techhub_multiple_retrieval",
        "params": {
            "retrieve": [
                {
                    "query": query_1,
                    "index": index,
                    "top_k": top_k
                },
                {
                    "query": query_2,
                    "index": index,
                    "top_k": top_k
                }
            ]               
        }
      }
    }
  }
}

headers = {
  "Content-type": "application/json",
  "x-api-key": "### ADD HERE API KEY"
}

response = requests.request("POST", url, headers=headers, data=payload)
~~~

* Only LLM without retrieval

~~~
import requests
import json

url = "https://<deploymentdomain>/compose/process"

system = "You are an expert assistant"
query = "what is the france capital ?"
template_name = "system_query"
platform = "azure"
model = "gpt-3.5-16k-pool-techhub-japan"

payload = {
  "generic": {
    "compose_conf": {
      "session_id": "651d79fa-9562-42d4-abc4-9e9addb17489",
      "template": {
        "name": "techhub_llm_model",
        "params": {
          "system": system,
          "query_llm": query,
          "platform": platform,
          "model": model,
          "template_name": template_name          
        }
      }
    }
  }
}

headers = {
  "Content-type": "application/json",
  "x-api-key": "### ADD HERE API KEY"
}

response = requests.request("POST", url, headers=headers, json=payload)
~~~

* Chatbot mode with persistence

~~~
import requests
import json

url = "https://<deploymentdomain>/compose/process"

headers = {
  "Content-type": "application/json",
  "x-api-key": "### ADD HERE API KEY"
}

system = "You are an expert assistant"
query = "What time of year is it best to go to Galicia?"
platform = "azure"
model = "gpt-3.5-16k-pool-techhub-japan"

payload = {
  "generic": {
    "compose_conf": {
      "session_id": "651d79fa-9562-42d4-abc4-00000000000",
      "template": {
        "name": "techhub_llm_model",
        "params": {
          "system": system,
          "query_llm": query,
          "platform": platform,
          "model": model,
          "template_name": template_name 
        }
      },
    "persist": {
        "type": "chat",
        "params": {
            "max_persistence": 10
        }
      }           
    }
  }
}

response = requests.request("POST", url, headers=headers, json=payload)

query = "How can I travel there?"
payload = {
  "generic": {
    "compose_conf": {
      "session_id": "651d79fa-9562-42d4-abc4-00000000000",
      "template": {
        "name": "techhub_llm_model",
        "params": {
          "system": system,
          "query_llm": query,
          "platform": platform,
          "model": model,
          "template_name": template_name 
        }
      },
    "persist": {
        "type": "chat",
        "params": {
            "max_persistence": 10
        }
      }           
    }
  }
}

response = requests.request("POST", url, headers=headers, json=payload)
response.text
~~~


## Deployment

### Files overview

Each component has the following files and folders structure:

- **Helm**: This folder contains the templates of helm to build the charts for deploy microservices.

- **IaC**: Infrastructure as Code. This folders contains all the required files to set and deploy the services like cloud storage, queues, etc with Terraform. Besides this folder contain scripts to create, copy or other utilities to prepare the deployment of the service. Finally, it contain an azure-pipeline.yml to create 'Artifact' in Azure Devops that will later be used in the release responsible for generating all the resources.

- **imagesbase**: Contains the base images required for the component, used to accelerate creation of images dockers of microservices. This folder contain azure-pipeline.yml to create the pipeline that it create and upload imagen docker to ACR with different tags. This images download and install the libraries generated in the folder **libraries**.

- **libraries**: Contains the specific libraries and SDKs for the component and its azure-pipeline.yml. This library is saved within Azure Devops artifact feed. 

- **services**: Contains the component code files.

### Requirements

- Azure suscription
- Cluster Kubernetes
- Globals Resources

### Resources Azure Devops

#### 1. Variable groups (Library)

All azure-pipeline.yml of the **imagesbase** and **libraries** contains variables that these variables are set by groups of variables, in Azure Devops this groups are set in Library menu.

This way, the azure-pipeline.yml does not have to be modified, in case or these variables need to modified, modify only the values of variables within group variables of Library if you must update something param.

#### 2. Pipelines

This section is used to generate both the docker images and upload them to the corresponding acr as well as to package and generate Artifact, which will later be necessary to upload in the deployment releases.


#### 3. Releases

This section is used to create two releases, one for IaC and another for service deployment. 

This will be the final step with which everything necessary to have the component, toolkit or solution available is created or deployed.

### Stages to deploy

#### 1. Create library

Microservices GenAI need to connect with different cloud resources. To connect to these resources they use a library that implements the connectors.

So you have to compile the library and save it in a feed. You have to create the pipeline of azure within the folder **libraries** and run in a branch master to compile and stored.

Before you have to create the *Library* necesary with the correct values. You can check the name of library and values in yml.

#### 2. Create images base

Microservices GenAI are created about imagenes base. This saves time when generating the final docker image of the microservice.

You have to create the pipeline with its corresponding yml within of folder **imagesbase**.

Before you have to create the *Library* necesary with the correct values. You can check the name of library and values in yml.

#### 3. Create image microservice and artifact helm

Each microservices GenAI have own docker image.

To create image, in the folder **services** there is azure-pipeline.yml with configuration of pipeline to create and storage this image.

Before you have to create the *Library* necesary with the correct values.

This pipeline create docker image and artifact with helm code to deploy in the release.

#### 4. Create artifact IaC

Each microservices GenAI have own IaC code in Terraform and Scripts.

To create artifact, in the folder **IaC** there is azure-pipeline.yml with configuration of pipeline to create this artifact that later you need to create release.

Before you have to create the *Library* necesary with the correct values.

#### 5. Create releases

Finally, to create cloud resources and deploy services you have to create two releases. These releases can have one or more stages.

1. Release IaC: This release is in charge of creating cloud and kubernetes resources. In IaC release you have two stages:

   - Terraform: Create all resources cloud in Azure and finally it create namespace and secrets of kubernetes recover of creation resources cloud.
   - Scripts: Copy configuration files, if this resource need load files of configuration, of the one storage of administration.

2. Release Service: This release is in charge of creating the services by docker images and deploy in cluster as microservices. This steep create all resources necesary, this resources can to be elasticsearch, microservices genAI, Keda objects...

The configuration of these releases ara attached to the release folder within of repository.


### Running the bare application in a Python runtime environment
if you deploy the repository in your local computer, these are several steps that you need to take

#### Indexing pipeline
The first step you need to take to run the indexing pipeline on your local machine is to set the following environment variables:

```json
"PROVIDER": "azure/aws", //Cloud storage and queue service provider
"STORAGE_DATA": "tenant-data", //Cloud storage bucket or blob to store datasets
"STORAGE_BACKEND": "tenant-backend", //Cloud storage bucket or blob to store process results
"SECRETS_PATH": "path to secrets", //Path to the secrets file
"Q_PREPROCESS_START": "" //Name of the queue of the preprocess-start subcomponent
"Q_PREPROCESS_EXTRACT": "" //Name of the queue of the preprocess-extract subcomponent
"Q_PREPROCESS_OCR": "" //Name of the queue of the preprocess-ocr subcomponent
"Q_PREPROCESS_END": "" //Name of the queue of the start-preprocess subcomponent
"Q_INFO_INDEXING": "" // Name of the queue for the indexing subcomponent
```

After that, you need to create an environment with Python 3.8 and install the required packages listed in the "requirements.txt" file:

```sh
pip install -r "**path to the requirements.txt file**"
```

Once everything above is configured, you need to run the main.py file from the integration-receiver subcomponent, and call the /process endpoint with body and headers similar to the following example:


```python
import requests
import json

url = "http://localhost:8888/process"

payload = {
  "index": "index_name",
  "operation": "indexing",
  "documents_metadata": {
    "doc1.pdf": {"content_binary": "doc encoded as base64"}
  },
  "response_url": "http://"
}

headers = {
  "x-api-key": "secret api key"
}

response = requests.request("POST", url, headers=headers, data=payload)
```

#### RAG pipeline

To get started with the service on your local machine, you need to have Retrieval, Compose and LLMAPI services running and a template stored in azure blob storage in the path "src/compose/templates".

The example template we are using is called "retrieval_llm". This compose template defines the flow of actions. In this case, we will call retrieval and llm.

The first step you need to take to run the indexing pipeline on your local machine is to set the following environment variables:

```json
"URL_LLM": "http://url_llm/predict",
"URL_RETRIEVE": "http://url_retrieve/process",
"PROVIDER": "azure",
"STORAGE_BACKEND": "tenant-backend",
"AZ_CONN_STR_STORAGE": "",
"SECRETS_PATH": "path to secrets",
"AWS_ACCESS_KEY": "",
"AWS_SECRET_KEY": "",
"REDIS_DB_SESSION" : "4",
"REDIS_HOST" : "",
"REDIS_PORT" : "",
"REDIS_PASSWORD" : "",
"LANGFUSE_SECRET_KEY": "",
"LANGFUSE_PUBLIC_KEY": "",
"LANGFUSE_HOST": "",
"DEFAULT_LLM_MODEL": ""
```

Create a python 3.8 environment and install the required libraries in with the "requirements.txt" file.

```sh
pip install -r "**path to the requirement.txt file**"
```

Then execute the compose main.py file:

```sh
python compose/main.py"
```
To then call the /process endpoint, you will just have to change the <deployment_url> to localhost and include these parameters in the headers:

```json

"headers": {
    "x-tenant": "",
    "x-department": "",
    "x-reporting": ""
}
```


#### Config files
This files will be stored in the backend storage instead of the data one (some components needs two storages backend and data)
##### LLM config files `src/LLM/`
###### LLM models `/conf/models_config.json`
This file is the most important one because it stores information about the different language models that are used in genai-llmapi to have a natural language interaction with the user (chat, prompt…) 
```json
{
    "LLMs": {
        "azure": [ 
            {
                "model": "genai-text-gpt35turbo",
                "model_type": "gpt-3.5-turbo",
                "max_input_tokens": 4096,
                "zone": "genai",
                "message": "chatGPT",
                "api_version": "2024-02-15-preview",
                "model_pool":[
                    "gpt-3.5-pool-europe",
                    "gpt-3.5-pool-ew-europe"           
                ]
            }
        ]  
    }
}
```
Each parameter for a model configuration is:
- **model**: name of the model. In azure platform will be the deployment name of the model
- **model_type**: defined by the user (same models must have the same model_type)
- **max_input_tokens**: maximum number of tokens accepted by the model as input
- **zone**: place where the model has been deployed (used to get the api-key in the secrets file)
- **message**: type of message that will be used in order to adapt the input to the model requirements. It could be:
  - chatClaude: Claude models with text capabilities
  - chatClaude3: Claude models with text and vision capabilities
  - chatGPT: ChatGPT models with text capabilities
  - chatGPT-v: ChatGPT with text and vision capabilities
  - dalle3: Dall-E 3 models (image generation)
- **api_version**: version of the api (model) that is being used
- **model_pool**: pools the model belongs to

###### Templates/prompts `/prompts/**.json`
Stored in "src/LLM/prompts", in this directory we store the files containing the prompt templates like the following. When LLMAPI is initialized, reads all the files in the directory and loads to memory all the templates, removing duplicates. The name refered in the call will be the name of the dict key (system_query, system_context...). Finally, the only available files are the ones in json format and that contains query on its name.
```json
{
    "system_query": { 
        "system": "$system",
        "user": "$query",
    },
    "system_context": { 
        "system": "$system",
        "user": "$context",
    },
    "tests_gptv_query": { 
        "system": "$system",
        "user":[
            {
                "type": "text",
                "text": "Answer the question based on the image below"
            },{
                "type": "image_url",
                "image_url": {
                    "url": "https://imagerul.com",
                    "detail": "high",
                }
            },
            $query
        ]
    },
    "system_query_and_context_plus": { 
        "system": "$system",
        "user": "Answer the following task based on the following 'context' or the history of the conversation. if the information is not in the context provided, responds 'I can not answer your question'. \nTask: '$query' \n\n######\n\nContext:\n$context \n\n######\n\nAnswer:",
    }
}
```
In this config file, each model (separated by platforms) need different parameters:
- **azure**:
  - **embedding_model_name**: name of the model, decided by the user and used to distinguish between models.
  - **embedding_model**: type of embedding model that uses the model
  - **azure_api_version**: version of the api (embedding model) that is being used
  - **azure_deployment_name**: deployment name of the embedding model in azure
  - **zone**: place where the model has been deployed (used to get the api-key in the secrets file)
  - **model_pool**: pools the model belongs to
- **huggingface**. This type of model is not deployed anywhere, so there is no region or pool to specify:
  - **embedding_model_name**: same as before
  - **embedding_model**: same as before
  - **retriever_model** (mandatory in huggingface models): model used when retrieving information (in hugging-face models normally are different)


##### Integration config files `src/integration/`
##### Models map `/search/models_map.json`
This file stores the information about the embedding models needed in the infoindexing queue message. The file looks like:

```json
{
  "dpr": {
    "alias": "dpr-encoder",
    "embedding_model": "sentence-transformers/facebook-dpr-ctx_encoder-single-nq-base",
    "platform": "huggingface"
  },
  "azure_openai_ada": {
    "alias": "ada-002-pool-europe",
    "embedding_model": "text-embedding-ada-002",
    "platform": "azure"
  }
}
```
The parameters are:
- **alias**: Model or pool of models to index (equivalent to *"embedding_model_name"* in *models_config.json* config file for infoindexing and inforetrieval)
- **embedding_model**: Type of embedding that will calculate the vector of embeddings (equivalent to *"embedding_model"* in *models_config.json* config file for infoindexing and inforetrieval)
- **platform**: Provider used to store and get the information (major keys in *models_config.json* config file for infoindexing and inforetrieval)

##### Inforetrieval + Infoindexing config files `src/ir/`
###### IR models `/conf/models_config.json`
This file stores information about the different embedding models that are used in genai-inforetrieval and genai-infoindexing to generate the embeddings that will be stored in the vector storage database.
```json
{
    "embeddings": {
       "azure": [
            {
                "embedding_model_name": "ada-002-genai-westeurope", 
                "embedding_model": "text_embedding-ada-002",
                "azure_api_version": "2022-12-01",
                "azure_deployment_name": "dolffia-text-ada",
                "zone": "genai-westeurope",
                "model_pool":[
                    "ada-002-pool-europe",
                    "ada-002-pool-world",
                ]
            }
        ],
        "huggingface": [ 
            {
                "embedding_model_name": "dpr-encoder", 
                "embedding_model": "sentence-transformers/facebook-dpr-ctx_encoder-single-nq-base",
                "retriever_model ": "sentence-transformers/facebook-dpr-question_encoder-single-nq-base"
            }
        ]  
    }      
}
```

###### Default embedding models `/conf/default_embedding_models.json`
This config file is needed to specify which model is going to be used when in the retrieval call, no models are passed and the retrieval is going to be done with all the embedding models used in indexation. The file is stored in "src/ir/conf" and it has to be based on the previous file. The bm25 field is mandatory as it's used always in the indexation process:
```json
{
  "bm25": "bm25",
  "*embedding_model*": "*embedding_model_name/pool*"
}
```
An example could be:
```json
{
  "bm25": "bm25",
  "text-embedding-ada-002": "ada-002-pool-europe",
  "cohere.embed-english-v3": "cohere-english-v3-america"

}
```

###### Different vector storage for an index `/index/**.json`
This file is optional, just if a concrete index is stored in a different vector_storage (reacheable by the application) with its credentials in the vector_storage_config secret explained below. The file is stored in "src/ir/index" and its name will be the same as the used in the indexation/retrieval (index_name.json) process. It looks like:
```json
{
    "vector_storage": "*vector_storage_alias*"
}
```

##### Compose config files `src/compose/`
###### Compose templates `/templates/**.json`
In these files, the actions steps to execute by the compose module are stored in json format. The different actions that can be executed are:
**Retrieve**

This is to retrieve indexed documents based on a query. This is the most important action, and it is the one that will define our entry. In most cases, there should always be a retrieval that will usually be the first step of the flow. Once the search results are obtained in the format defined by the data model: By defauld, 1 streamlist with several streamchunk segments, other actions can be applied to them. It is also possible to store the chunks in different streamlists within the streambatch. An example json template:
```json
{
    "action": "retrieve",
    "action_params": {
        "params": {
            "generic": {
                "index_conf": {
                    "index": "$index",
                    "query": "$query",
                    "task": "retrieve",
                    "top_k": 5,
                    "filters": $filters
                },
                "process_type": "retrieve",
            }
        },
        "type": "get_chunks"
    }
}
```
Parameters of the 'retrieve' action:
- **Index (string):** Index name where the document to retrieve is stored.
- **Query (string):** User query to execute.
- **Top_k (int):** Number of chunks to retrieve.
- **Filters (json/dict):** Filters to apply while retrieving the document chunks.
- **Type (string):** Retrieve type. The available types are:
  - "get_chunks": Calls genai-inforetrieval to get the K number of chunks from the specified index.
  - "get_document": Calls genai-inforetrieval to get the entire document content specified.

With the same action in the template json file, we can execute multiple retrieval to obtain different streamlist with different queries using the "retrieve" action name in the json api call.

**Filter** 

This action is executed at the streamlist level. The aim is to apply a filter to the streamlist (that is, to the list of retrieved documents). For example, you could apply a filter to keep only 5 chunks  out of the 10 chunks that the genai-inforetrieval returned. Another example would be to filter those documents that contain a certain metadata or that are not really related to the input query. Parameters of the action 'filter:
  - **Type (string):** Filter type to execute. (top_k, related_to, metadata, permissionfilter)
  - **Top_k (int):** Number of chunks to return.
  - **Model (string):** LLM model to use.
  - **Platform (string):** Platform hosting the LLM.
  - **Query (string)**
  - **Template_name (string):** Template name to use while calling genai-llmapi.
  - **Filer_conditions (json/dict):** Conditions to check in the retrieved chunks to filter them. Using “or”, “and” to combine conditions and each condition is structured the same way, {“condition type”: {“metadata name”: “condition”}}

Example of filter action using type “related_to”:
```json
{
    "action": "filter",
    "action_params": {
        "params": {
            "llm_metadata": {
                "model": "gpt-3.5-16k-pool-techhub-europe",
            },
            "platform_metadata": {
                "platform": "azure",
            },
            "query_metadata": {
                "query": "$query",
                "template_name": "query_and_context_related",
            },
        },
        "type": "related_to"
    }
}
```
Example of filter action using type “metadata”:
```json
{
    "action": "filter",
    "action_params": {
        "params": {
            "filter_conditions": {
                "or": [
                    {"eq": ("city","New York")}, //Checks if a metadata is equal to a value.
                    {"in": ("city",["New York","London"])}, //Checks if a metadata is in a list of values.
                    {"textinmeta": ("city","New Yor")}, //Checks if a string is contained in the metadata.
                    {"metaintext": ("city","The city of New York is known as the big 
                                apple")}
                ],
                "and": [
                    {"gt": ("age",30)}, //Checks if a metadata is greater than a value.
                    {"lt": ("age",40)}, //Checks if a metadata is lower than a value 
                    {"gt_date": ["date","2023-12-17"]}, //Greater than variation to work with date values
                    {"lt_date": ["date","2023-12-19"]} //Lower than variation to work with date values
                ],
            },
            "operator": "and" //Operator to combine the previous group of filters. 
          },
        "type": "related_to"
    }
}
```

Allowed date types:
- Yyyy-mm-dd
- Yyyy/mm/dd
- Yyyy/mm
- yyyy
- Yyyymmdd
- Mmddyy
- Mmddyyyy
- Mm/dd/yy
- Mm/dd/yyyy
- Mm-dd-yy
- Mm-dd-yyyy

**Merge**

This action merges the different streamchunks in a streamlist into a single streamchunk. Starts with 1 streambatch containing 1 streamlist with multiple streamchunks and it ends with a streambatch containing 1 streamlist with the merged content in 1 chunk. It is also possible to set a grouping key to get the result in different streamchunks, 1 streamchunk per group. The result chunk will have the merged information in the content field and in the metadata common to all the chunks merged will be saved in the new chunk.

The parameters of this action are:
- Template (string): Template to used to set the structure of the result content, the words starting with “$” represents the value of metadata or attribute of each chunk.
- Sep (string): Value to use to separate each content chunk.
- Grouping_key (string): Value to group the results.
                    
Example for action 'merge' in template:
```json
{
    "action": "merge",
    "action_params": {
        "params": {
            "template ": "Content: $content, Date: $date, Doc name: 
                         $document_name",
            "sep": "####"
        },
        "type": "meta"
    }
}
```

**Batchmerge**

This action merges the different streamlist in a streambatch into a single streamlist with non-repeated chunks. Starts with 1 streambatch containing multiple streamlist containing multiple chunks that can be repeated between streamlists and it ends with a streambatch containing 1 streamlist with unique chunks.        

Example for action 'batchmerge' in template:
```json
{
    "action": "batchmerge",
    "action_params": {
        "params": {
        },
        "type": "add"
    }
}
```

**Sort**

Can be executed for the streambatch or for streamlist. It can sort the streamlist based on the score, content length, document id or snippet number and the streambatch based on the mean score or the overall content. It can also sort based on other specified metadata or date. The usable date formats are the same as for the 'filter' action.

The parameters of this action are:
- **Type (string):** Sorting type to execute.
  - Score: Sorts by the mean score of each chunk.
  - Length: Sorts by the length of each chunk.
  - Doc_id: Sort by the document id of each chunk.
  - Sn_number: Sort by the snippet number of each chunk.
  - Date: Sort by metadata named “date” with date type values.
  - Meta: Sort by the metadata value, date values don’t work in this type.
- **Desc (bool):** Sort descendant or ascendant.
- **Value:** Metadata to use while sorting the streamlist.

Example for action 'sort' with type length:
```json
{
    "action": "sort",
    "action_params": {
        "params": {
            "desc": true
        },
        "type": "length"
    }
}
```

Example for action 'sort' with type meta:
```json
{
    "action": "sort",
    "action_params": {
        "params": {
            "desc": true,
            "value": $metadata_name
        },
        "type": "meta"
    }
}
```

**Groupby**

This action sorts the streamlist by groups. Each group will be sorted by snippet_number, like its natural order and then the groups can be sorted by the maximum score from each group, the mean score from each group and by date.

The parameters of this action are:
  - **Type (string):** Groupby type to use. (docscore, date).
  - **Method (string):** Method to use in the docscore sorting (max, mean).
  - **Desc (bool):** Sort descendant or ascendant.

Example for action ‘groupby’ with type docscore:
```json
{
    "action": "groupby",
    "action_params": {
        "params": {
            "desc": true,
            "method": "max"
        },
        "type": "docscore"
    }
}
```

Example for action ‘groupby’ with type date:
```json
{
    "action": "groupby",
    "action_params": {
        "params": {
            "desc": true
        },
        "type": "date"
    }
}
```

**LLM_action**

This is the action that calls the LLM service (that is, it calls an LLM). This is the action where the LLM template (“template_name”) must be defined. Here we can also define the system and query that will be sent to the LLM.
The parameters of this action are:
- **Type (string):** Method to user while calling genai-llmapi. (llm_content, llm_segments)
- **Model (string):** LLM model to use.
- **Platform (string):** Platform hosting the LLM.
- **Query (string)**
- **Template_name (string):** Template name to use while calling genai-llmapi.
- **System (string):** Context and task that will be sent to the LLM.

Within this action, there are two types:
- **Llm_content:** This gets all the document fragments retrieved and merges them into a single fragment that then is sent to the LLM. It returns a single response with a streambatch of one streamlist containing all the chunks retrieved and the and the last element of the streamlist will be the answer generated by the LLM.

Example for ‘llm_action’ action with type llm_content
```json
{
    "action": "llm_action",
    "action_params": {
        "params": {
            "llm_metadata": {
                "model": "gpt-3.5-16k-pool-techhub-europe",
                "max_input_tokens": 5000
            },
            "platform_metadata": {
                "platform": "azure",
            },
            "query_metadata": {
                "query": "$query",
                "system": "You are a helpful assistant",
                "template_name": "system_query_and_context_plus",
            },
        },
        "type": "llm_content"
    }
}
```

- **Llm_segments:** This takes each one of the document fragments and sends them individually to the LLM. Therefore, you will get as many responses as document fragments you sent. The response will contain a streambatch of one streamlist containing the chunks retrieved with each answer.

Example for ‘llm_action’ action with type llm_segment
```json
{   
    "action": "llm_action",
    "action_params": {
        "params": {
            "llm_metadata": {
                "model": "gpt-3.5-16k-pool-techhub-europe",
                "max_input_tokens":5000
            },
            "platform_metadata": {
                "platform": "azure"
            },
            "query_metadata": {
                "query": "$query",
                "system":"You are a helpful assistant",
                "template_name": "system_query_and_context_plus"
            }
        },
        "type": "llm_segment"
    }
}
```

#### Secrets
    
All necessary credentials for the components are stored in secrets for security reasons. These secrets are JSON files that must be located under a common path defined by the [environment variable](#environment-variables) 'SECRETS_PATH'; the default path is "secrets/". Within this secrets folder, each secret must be placed in a specific subfolder (these folder names are predefined). The different secrets that are used in the components are:

- **Vector storages configuration**: file where data like credentials, url... from the different vector_storages supported are stored (currently, only ElasticSearch is supported). The custom partial path for this file is "vector-storage/" making the full path `"secrets/vector-storage/vector_storage_config.json"`.. The format of the secret is as follows:
    ```json
    {
        "vector_storage_supported": [{
                "vector_storage_name": "elastic-develop",
                "vector_storage_type": "elastic",
                "vector_storage_host": "*host*",
                "vector_storage_schema": "https",
                "vector_storage_port": 9200,
                "vector_storage_username": "elastic",
                "vector_storage_password": "*password*"
            },
            . . .
        ]
    }
    ```

    The different parameters (only for elastic as is the available one) are:
    - **vector_storage_name:** Alias of the vector storage to be identified. (must match with the environment variable VECTOR_STORAGE)
    - **vector_storage_type:** Type of the vector storage selected (currently, only "elastic" is allowed).
    - **vector_storage_host:** Host of the vector storage
    - **vector_storage_schema:** Schema of the vector storage
    - **vector_storage_port:** Port where the vector storage is located.
    - **vector_storage_username:** Username to access to the vector storage
    - **vector_storage_password:** Password to access to the vector storage

- **Models api-keys and urls**: file where urls and api-keys from the models are stored. This fields are separated, because api-keys are shared by the models for each region and the url's are always the same for a same type of models. The custom path for this secret is "models/", making the full path `"secrets/models/models.json"`. The secret looks like:
    ```json
    {
        "URLs": {
            "AZURE_EMBEDDINGS_URL": "https://$ZONE.openai.azure.com/",
            "AZURE_DALLE_URL": " https://$ZONE.openai.azure.com/openai/deployments/$MODEL/images/generations?api-version=$API",
            "AZURE_GPT_CHAT_URL": "https://$ZONE.openai.azure.com/openai/deployments/$MODEL/chat/completions?api-version=$API",
            "OPENAI_GPT_CHAT_URL": "https://api.openai.com/v1/chat/completions"
        },
        "api-keys": {
            "azure": {
                "*zone*": "*api-key*",
            }
        }
    }
    ```
    The explanation for every field:
    - The **URLs** field has all urls of the available models. For LLMApi models the urls must be inside the code in order to replace the "$ZONE", "$MODEL" and "$API" params obtained from "models_config.json" because all the base of the urls from azure is always the same. 
    - The **api-keys** field is to provide the api-keys of the models. in OpenAI the same api_key is shared for all of the models, in azure depends on its region
## Advanced Examples

To run the following examples, change the payload JSON to the ones indicated below and call the system with that JSON.

### Indexing Pipeline

#### Call indexing with one document and default metadata

This example illustrates the simplest request: the indexing of a single document in the vector database, with default model, parameters and metadata.

```json
{
  "index": "index_example1",
  "operation": "indexing",
  "documents_metadata": {"doc1.txt": {"content_binary": "aG9sYQ=="}},
  "response_url": "http://",
}
```

Given the asynchronous nature of this service, the response of this request will be as follows:

```json
{
  "status": "processing",
  "request_id": "request_20240627_134044_348410"
}
```

#### Call indexing with custom metadata

Users often need to assign custom metadata to the documents in order to perform more precise searches, since these can be used as filters in the retrieval of documents or text chunks by the search system. In this example, the custom metadata provided in the request is incorporated in all indexed chunks of text. The body of the request is as follows:

```json
{
  "index": "index_example2",
  "operation": "indexing",
  "documents_metadata": {"doc1.txt": {"content_binary": "aG9sYQ=="}},
  "response_url": "http://",
  "metadata": {"document_id": "123abc", "section": "technology"}
}
```

Again, the response for the asynchronous process is as follows:

```json
{
  "status": "processing",
  "request_id": "request_20240627_103045_632178"
}
```

#### Call indexing with custom parameters

Apart from the documents metadata, parameters and embedding models can also be customized to adapt the indexation process to different use case requirements. The request would be as follows:

```json
{
  "index": "index_example3",
  "operation": "indexing",
  "documents_metadata": {"doc1.txt": {"content_binary": "aG9sYQ=="}},
  "response_url": "http://",
  "metadata": {"document_id": "123abc", "section": "technology"},
  "window_length": 700,
  "models": ["azure_openai_ada"]
}
```

Response:

```json
{
  "status": "processing",
  "request_id": "request_20240627_103625_492091"
}
```

#### Call indexing with request_id and response_url

Since the indexing process is asynchronous, having an accessible endpoint where the system can report the completion of the process may be beneficial. For this purpose, it is possible to add a request_id and a callback url to the indexing request:

```json
{
  "request_id": "request_20240627_104218_291084",
  "index": "index_example4",
  "operation": "indexing",
  "documents_metadata": {"doc1.txt": {"content_binary": "aG9sYQ=="}},
  "response_url": "http://example-callback.com/end-process",
}
```

Response:

```json
{
  "status": "processing",
  "request_id": "request_20240627_104218_291084"
}
```

Once the process ends, the accesible endpoint provided will receive a POST request with the folowing body:

```json
{
  "status": "Finished",
  "request_id": "request_20240627_104218_291084",
  "index": "index_example4",
  "docs": "doc1.txt"
}
```

#### Update indexed documents

Once documents have been indexed, it is often necessary to update their information. In this service, you can achieve this by including the `modify_index_docs` parameter, which specifies the metadata used to filter the document for editing. The request body is as follows:

```json
{
  "index": "index_example5",
  "operation": "indexing",
  "documents_metadata": {"doc1.txt": {"content_binary": "aG9sYQ=="}},
  "response_url": "http://",
  "modify_index_docs": {"update": {"filename": true}}
}
```

Response:

```json
{
  "status": "processing",
  "request_id": "request_20240627_105502_491023"
}
```

### RAG Pipeline

#### Retrieve document by metadata and call LLM

In this case we want to retrieve a document or documents from our index depending on its metadata (not on the query as before) to be sent as context to the LLM. The request to retrieve one document by its filename would be:

```json
{
    "generic": {
        "compose_conf": {
            "template": {
                "name": "retrieve_docs_and_llm",
                "params": {
                    "query": "What are the New Year's resolutions?",
                    "index": "myindex",
                    "filters": {
                        "filename": "manual.docx"
                    }
                }
            }
        }
    }
}
```

If we want to retrieve more information we can do a multiple retrieval. In the following request one retrieval will filter by 'filename' and the other by 'snnipet_id'.

```json
{
    "generic": {
        "compose_conf": {
            "template": {
                "name": "retrieve_docs_and_llm",
                "params": {
                    "query": "What is the purpose of compose?",
                    "retrieve": [
                        {
                            "index": "myindex",
                            "filters": {
                                "filename": "manual.docx"
                            }                        
                        },
                        {
                            "index": "myindex2",
                            "filters": {
                                "snippet_id": "871d4f43-49c5-47a6-8d10-c8681cec7589"
                            }                         
                        }
                    ]
                }
            }
        }
    }
}
```

As before, the retrieved information will be sent to the LLM as context to answer the query "What are the New Year's resolutions?".

For these last two requests the template "retrieve_docs_and_llm" would be:

```json
[
    {
        "action": "retrieve",
        "action_params": {
            "params": {
                "index": "$index",
                "filters": $filters
            },
            "type": "get_documents"
        }
    },
    {
        "action": "llm_action",
        "action_params": {
            "params": {
                "llm_metadata": {
                    "model": "gpt-3.5-16k-pool-techhub-japan",
                    "max_input_tokens":5000
                },
                "platform_metadata": {
                    "platform": "azure"
                },
                "query_metadata": {
                    "query": "$query",
                    "system":"You are a helpful assistant",
                    "template_name": "system_query_and_context_plus"
                }
            },
            "type": "llm_content"
        }
    }
]
```

In the "retrieve_docs_and_llm" compose template, the retrieve action has a different "type" than in the "techhub_retrieval_reference" template.
The type here is "get_documents" that retrieves the entire document, whereas in the "techhub_retrieval_reference" the type is "get_chunks" that retrieves chunks.

Response:

```json
{
    "status": "finished",
    "result": {
        "session_id": "username/my_session_id",
        "streambatch": [
            [
                {
                    "content": "| |  | \Application Generative AI – Manual | \Application Generative AI – Manual | \Application Generative AI – Manual | \v Generative AI – Manual |  | \n|\nSummary of the documentation\The application is an accelerator platform for solving use cases that utilize generative AI. As an accelerator, it provides a set of tools and functionalities (through API) to configure and develop upon. The goal is to have common functionalities and components for generative AI use cases. \nThere are two main pipelines: index manager and the compose manager. The index manager is responsible for taking documents that a user inputs and saving them into the database, including all the intermediate steps to extract information from them. The compose manager is responsible for taking a user query and conducting the generative AI tasks of the use case to return a response.\nIn this document, there are examples of the different templates that can be done for different uses cases. ...",
                    "meta": {
                        "document_id": "e72ac490-27c4-49db-8114-4fa26e6833d2",
                        "filename": "manual.docx",
                        "sections_headers": "",
                        "snippet_id": "253ca09e-ba23-4c33-b52c-3a9fc5637170",
                        "snippet_number": 0,
                        "tables": "",
                        "uri": "https://d2astorage.blob.core.windows.net/uhis-cdac-develop-.../manual.docx"
                    },
                    "scores": {},
                    "answer": null
                },
                {...},
                {...},
                {
                    "content": "",
                    "meta": {
                        "title": "Summary"
                    },
                    "scores": 1,
                    "answer": "The purpose of the compose manager is to take a user query and perform generative AI tasks related to the use case at hand. It conducts these tasks on the indexed documents and returns a response based on the user query.",
                    "tokens": {
                        "input_tokens": 1448,
                        "output_tokens": 7
                    }
                }
            ],
            [
                {
                    "content": "This can be just text or an image in format url or base64. The options for this key are: text/image_url/image_b64\nText: If the type is “text” this key is mandatory and it contains a string with the question/text to send to the llm.\nUrl: If the type is “image_url” this key is mandatory, and it contains a string with the url to the image.\nBase64: If the type is “image_b64” this key is mandatory and it contains a base64 string encoding a image.\n\nThe following would be an example of a query containing the three types:\n\"query\": [\n   {\n     \"type\": \"text\",\n     \"text\": \"How old is OpenAI?\"\n   },\n   {\n     \"type\": \"image_url\",\n     \"url\": \"https://imagelink.jpg”\n   },\n   {\n     \"type\": \"image_b64\",\n     \"base64\": \"base64stringencodingimage”\n   }\n]\nImages formats allowed: jpeg, png, gif and webp. GPT vision models have a maximum of 10 allowed images and Claude3 allows a maximum of 20 images.\n\nPrompting\nPrompt engineering in generative AI is a fundamental technique for achieving effective and relevant responses in conversations with models such as GPT 3.5 Turbo.",
                    "meta": {
                        "document_id": "e72ac490-27c4-49db-8114-4fa26e6833d2",
                        "filename": "manual.docx",
                        "sections_headers": "",
                        "snippet_id": "871d4f43-49c5-47a6-8d10-c8681cec7589",
                        "snippet_number": 0,
                        "tables": "",
                        "uri": "https://d2astorage.blob.core.windows.net/uhis-cdac-develop-.../manual.docx"
                    },
                    "scores": {},
                    "answer": null
                },
                {...},
                {...},
                {
                    "content": "",
                    "meta": {
                        "title": "Summary"
                    },
                    "scores": 1,
                    "answer": "The purpose of \"compose\" is to provide the necessary information for generating a response in conversations with models like GPT 3.5 Turbo. It includes specifying the type of content (text, image URL, or base64-encoded image) and providing the corresponding data. Prompt engineering is also mentioned as a technique for achieving better responses.",
                    "tokens": {
                        "input_tokens": 1448,
                        "output_tokens": 7
                    }
                }
            ]
        ],
        "answer": "The purpose of the compose manager is to take a user query and perform generative AI tasks related to the use case at hand. It conducts these tasks on the indexed documents and returns a response based on the user query."
    },
    "status_code": 200
}
```

#### Retrieve full document
  
If we want to retrieve a whole document or documents from our index depending on its metadata without calling the LLM we must eliminate the key 'query' from the request 'params'. Unlike the previous case where we receive the document in many chunks, in this case we will receive the document with all the text in a row.

```json
{
    "generic": {
        "compose_conf": {
            "template": {
                "name": "retrieve_fulldocument",
                "params": {
                    "retrieve": [
                        {
                            "index": "myindex",
                            "filters": {
                                "filename": "manual.docx"
                            }                        
                        }
                    ]
                }
            }
        }
    }
}
```

Keeping just the 'retrieve' action with type "get_documents" the template looks like:

```json
[
    {
        "action": "retrieve",
        "action_params": {
            "params": {
                "index": "$index",
                "filters": $filters
            },
            "type": "get_documents"
        }
    }
]
```

This way we will receive the whole documents "myfile.pdf" and "myfile2.pdf" from the indexes "myindex" and "myindex2" respectively.

When using the type "get_documents" in the retrieve action it is mandatory to send the filters parameter not empty.

Response:

```json
{
    "status": "finished",
    "result": {
        "session_id": "username/my_session_id",
        "streambatch": [
            [
                {
                    "content": "| |  | \Application Generative AI – Manual | \Application Generative AI – Manual | \Application Generative AI – Manual | \Application Generative AI – Manual |  | \n|\nSummary of the documentation\The application is an accelerator platform for solving use cases that utilize generative AI. As an accelerator, it provides a set of tools and functionalities (through API) to configure and develop upon. The goal is to have common functionalities and components for generative AI use cases. \nThere are two main pipelines: index manager and the compose manager. The index manager is responsible for taking documents that a user inputs and saving them into the database, including all the intermediate steps to extract information from them. The compose manager is responsible for taking a user query and conducting the generative AI tasks of the use case to return a response.\nIn this document, there are examples of the different templates that can be done for different uses cases. ...",
                    "meta": {
                        "document_id": "e72ac490-27c4-49db-8114-4fa26e6833d2",
                        "filename": "manual.docx",
                        "sections_headers": "",
                        "snippet_id": "253ca09e-ba23-4c33-b52c-3a9fc5637170",
                        "snippet_number": 0,
                        "tables": "",
                        "uri": "https://d2astorage.blob.core.windows.net/uhis-cdac-develop-.../manual.docx"
                    },
                    "scores": {},
                    "answer": null
                },
                {...},
                {...},
                {...}
            ]
        ],
        "answer": null
    },
    "status_code": 200
}
```

#### Using actions

##### Expand query action

Using the expand query action can be combined with batchmerge to unify all the retrieved chunks for the llm:

```json
{
  "generic": {
    "compose_conf": {
      "template": {
        "name": "retrieve_expand_query",
        "params": {
            "index": "myindex",
            "query": "What does the sort action do?",
            "langs": ["es", "ja"]
        }
      }
    }
  }
}
```

Template:

```json
[
    {
        "action": "expansion",
        "action_params":{
            "params": {
                "langs" : "$langs"
            },
            "type": "lang"
        }
    },
    {
        "action": "retrieve",
        "action_params": {
            "params": {
                "generic": {
                    "index_conf": {
                        "add_highlights": false,
                        "index": "$index",
                        "query": "$query",
                        "task": "retrieve",
                    },
                    "process_type": "ir_retrieve"
                }
            },
            "type": "get_chunks"
        }
    },
    {
        "action": "batchmerge",
        "action_params": {
            "params": {
            },
            "type": "add"
        }
    },
    {
        "action": "llm_action",
        "action_params": {
            "params": {
                "llm_metadata": {
                    "model": "gpt-3.5-16k-pool-techhub-europe",
                    "max_input_tokens":5000
                },
                "platform_metadata": {
                    "platform": "azure"
                },
                "query_metadata": {
                    "query": "$query",
                    "system":"You are a helpful assistant",
                    "template_name": "system_query_and_context_plus"
                }
            },
            "type": "llm_content"
        }
    }
]
```

Response

```json
{
    "status": "finished",
    "result": {
        "session_id": "username/my_session_id",
        "streambatch": [
            [
                {
                    "content": "Example json template:\n{\n    \"action\": \"batchmerge\",\n    \"action_params\": {\n        \"params\": {\n        },\n        \"type\": \"add\"\n    }\n}\nType (string): Batchmerge type to execute. (add)\n\nSort. Can be executed for the streambatch or for streamlist. It can sort the streamlist based on the score, content length, document id or snippet number and the streambatch based on the mean score or the overall content. It can also sort based on other specified metadata or date. The usable date formats are: \nYyyy-mm-dd\nYyyy/mm/dd\nYyyy/mm\nyyyy\nYyyymmdd\nMmddyy\nMmddyyyy\nMm/dd/yy\nMm/dd/yyyy\nMm-dd-yy\nMm-dd-yyyy\n\nExample json template 1:\n{\n    \"action\": \"sort\",    \n    \"action_params\": {\n        \"type\":  \"length\",\n        \"params\":  {\n            \"desc\":  true\n        }\n    }\n}\nType (string): Sorting type to execute. \nScore: Sorts by the mean score of each chunk.\nLength: Sorts by the length of each chunk.\nDoc_id: Sort by the document id of each chunk.\nSn_number: Sort by the snippet number of each chunk.\nDate: Sort by metadata named “date” with date type values.",
                    "meta": {
                        "uri": "https://d2astorage.blob.core.windows.net/uhis-cdac-develop-dataivandegregoriougarte/request_20240916_101923_083524_x2pm4n/manual.docx",
                        "sections_headers": "",
                        "tables": "",
                        "filename": "manual.docx",
                        "document_id": "86b54b9c-0b72-4daa-97ff-ff238b7e2bd2",
                        "snippet_number": 30,
                        "snippet_id": "939e092c-0ec3-4eb7-bf90-dbd8c9962a67"
                    },
                    "scores": {
                        "text-embedding-ada-002--score": 0.8743681,
                        "bm25--score": 0
                    },
                    "answer": null,
                    "tokens": null
                },
                {...},
                {...},
                {...},
                {
                    "content": "{\n  \"action\": \"sort\",     \n  \"action_params\": {\n    \"type\":  \"score\",\n      \"params\":  {\n       \"desc\":  true\n       }\n   }\n}\n{\n  \"action\": \"sort\",     \n  \"action_params\": {\n    \"type\":  \"meta\",\n      \"params\":  {\n       \"desc\":  true,\n       “value”: metadata_name\n       }\n   }\n}\n\n\nCompose – Load session to REDIS\nWith this endpoint, the user can store sessions to REDIS to use them with the compose. The endpoint is called “https://api.dev.dolffia.com/compose/load_session” and it needs a json request like this:\n{\n    \"session_id\": \"session_example123\",\n    \"conv\": [\n        {\n            \"user\": \"Quien es Fernando Alonso?\",            \n            \"assistant\": \"Fernando Alonso es un reconocido piloto de automovilismo español.\"\n        }\n   ]\n}\n\nThe response would be:\n{\"status\": \"finished\", \"result\": \"Session <session_example123> saved in redis\", \"status_code\": 200}\n\nIt requires a “session_id” as the key and the param “conv” with the conversation to store in REDIS. The conversation is a list containing dictionaries formed with {“user”: “query from the user”, “assistant”: “Response from the LLM assistant”}.",
                    "meta": {
                        "uri": "https://d2astorage.blob.core.windows.net/uhis-cdac-develop-dataivandegregoriougarte/request_20240916_101923_083524_x2pm4n/manual.docx",
                        "sections_headers": "",
                        "tables": "",
                        "filename": "manual.docx",
                        "document_id": "86b54b9c-0b72-4daa-97ff-ff238b7e2bd2",
                        "snippet_number": 69,
                        "snippet_id": "9d77194c-3a8c-44da-8b98-06035705f3aa"
                    },
                    "scores": {
                        "bm25--score": 0.636819615625834,
                        "text-embedding-ada-002--score": 0.85469764
                    },
                    "answer": null,
                    "tokens": null
                },
                {
                    "content": "",
                    "meta": {
                        "title": "Summary"
                    },
                    "scores": 1,
                    "answer": "The \"sort\" action is used to arrange or order a streamlist or streambatch based on certain criteria. It can sort the streamlist based on the score, content length, document id, snippet number, or other specified metadata or date. For the streambatch, it can be sorted based on the mean score or overall content. The available types for sorting are \"score\", \"length\", \"doc_id\", \"sn_number\", and \"meta\". The \"desc\" parameter can be used to select between descending and ascending order.",
                    "tokens": {
                        "input_tokens": 2355,
                        "output_tokens": 107
                    }
                }
            ]
        ],
        "answer": "The \"sort\" action is used to arrange or order a streamlist or streambatch based on certain criteria. It can sort the streamlist based on the score, content length, document id, snippet number, or other specified metadata or date. For the streambatch, it can be sorted based on the mean score or overall content. The available types for sorting are \"score\", \"length\", \"doc_id\", \"sn_number\", and \"meta\". The \"desc\" parameter can be used to select between descending and ascending order."
    },
    "status_code": 200
}
```

##### Filter action

To use the filter action with type "top_k" for the example we need to add the parameter "top_k":

```json
{
  "generic": {
    "compose_conf": {
      "template": {
        "name": "retrieve_filter_llm",
        "params": {
            "index": "myindex",
            "query": "What does the sort action do?",
            "top_k": 2,
            "llm_template":"system_query_and_context_plus",
            "$model": "gpt-3.5-16k-pool-techhub-japan",
            "platform": "azure"
        }
      }
    }
  }
}
```

Now we add the 'filter' action to the template. The template "retrieve_filter_llm" is:

```json
[
    {
        "action": "retrieve",
        "action_params": {
            "params": {
                "generic": {
                    "index_conf": {
                        "add_highlights": false,
                        "index": "$index",
                        "query": "$query",
                        "task": "retrieve",
                        "top_k": "$top_k",
                        "filters": "$filters"
                    },
                    "process_type": "ir_retrieve"
                }
            },
            "type": "get_chunks"
        }
    },
    {
        "action": "filter",
        "action_params":  {
            "type":  "top_k",
            "params":  {
                "top_k": "$top_k"
            }
        }
    },
    {
        "action": "llm_action",
        "action_params": {
            "params": {
                "llm_metadata": {
                    "model": "$model", 
                    "max_input_tokens":5000
                },
                "platform_metadata": {
                    "platform": "$platform"
                },
                "query_metadata": {
                    "query": "$query",
                    "system":"You are a helpful assistant",
                    "template_name": "$llm_template"
                }
            },
            "type": "llm_content"
        }
    }
]
```

Response:

```json
{
    "status": "finished",
    "result": {
        "session_id": "username/my_session_id",
        "streambatch": [
            [
                {
                    "content": "{\n    \"generic\": {\n        \"compose_conf\": {\n            \"template\": {\n                \"name\": \"retrieval_sort”,\n                \"params\": {\n                    \"query\": \"What are the New Year's resolutions?\",\n                    \"index\": \"myindex\",\n                    \"sort_desc\": true,\n                    \"sort_type\": \"sn_number\"\n                }\n            }\n        ,\n            \"persist\": {\n                \"type\": \"chat\",\n                \"params\": {\n                \"max_persistence\": 20\n                }\n            }\n    }}\n}\n\nAnd how the template \"retrieval_sort” would look like:\n[\n  {\n    \"action\":  \"retrieve\",\n    \"action_params\":  {\n      \"params\":  {\n        \"generic\":  {\n          \"index_conf\":  {\n            \"add_highlights\":  false,\n            \"index\":  \"$index\",\n            \"query\":  \"$query\",\n            \"task\":  \"retrieve\",\n            \"top_k\":  20,\n            \"filters\":  $filters\n          },\n          \"process_type\":  \"ir_retrieve\"\n        }\n      },\n      \"type\":  \"get_chunks\"\n    }\n  },\n   {\n        \"action\": \"sort\",\n        \"action_params\": {\n            \"params\": {\n                \"desc\": $sort_desc\n            },",
                    "meta": {
                        "uri": "https://d2astorage.blob.core.windows.net/uhis-cdac-develop-.../manual.docx",
                        "sections_headers": "",
                        "tables": "",
                        "filename": "manual.docx",
                        "document_id": "39656327-0e14-4a6c-8e08-92319f0b7fca",
                        "snippet_number": 0,
                        "snippet_id": "6f143c24-2982-4740-bb7e-cb8d35b43ff9"
                    },
                    "scores": {
                        "bm25--score": 0.7274390585951781,
                        "text-embedding-ada-002--score": 0.9480624799999999
                    },
                    "answer": null
                },
                {...},
                {
                    "content": "",
                    "meta": {
                        "title": "Summary"
                    },
                    "scores": 1,
                    "answer": "The \"sort\" action is used to sort the streamlist of documents. It can sort by different criteria such as document id, snippet number, date, or metadata value. The sorting can be done in ascending or descending order. Additionally, the \"groupBy\" action can be used to sort the streamlist by groups based on different criteria.",
                    "tokens": {
                        "input_tokens": 1448,
                        "output_tokens": 7
                    }
                }
            ]
        ],
        "answer": "The \"sort\" action is used to sort the streamlist of documents. It can sort by different criteria such as document id, snippet number, date, or metadata value. The sorting can be done in ascending or descending order. Additionally, the \"groupBy\" action can be used to sort the streamlist by groups based on different criteria."
    },
    "status_code": 200
}
```

##### Merge action

The compose request will be "base_request" changing the query to "What are the components of compose?" and the actions template "retrieve_merge_llm" is as follows:

Body:

```json
{
  "generic": {
    "compose_conf": {
      "template": {
        "name": "retrieve_merge_llm",
        "params": {
            "index": "my index",
            "query": "What are the components of compose?",
            "top_k": 15,
            "model": "gpt-3.5-16k-pool-techhub-japan",
            "platform": "azure",
            "llm_template": "system_query_and_context_plus"
        }
      }
    }
  }
}
```

Template:

```json
[
    {
        "action": "retrieve",
        "action_params": {
            "params": {
                "generic": {
                    "index_conf": {
                        "add_highlights": false,
                        "index": "$index",
                        "query": "$query",
                        "task": "retrieve",
                        "top_k": "$top_k",
                        "filters": $filters
                    },
                    "process_type": "ir_retrieve"
                }
            },
            "type": "get_chunks"
        }
    },
    {
        "action": "merge",
        "action_params": {
            "params": {
                "template": "Content: $content",
                "sep": "###"
            },
            "type": "meta"
        }
    },
    {
        "action": "llm_action",
        "action_params": {
            "params": {
                "llm_metadata": {
                    "model": "$model",
                    "max_input_tokens": 5000
                },
                "platform_metadata": {
                    "platform": "$platform"
                },
                "query_metadata": {
                    "query": "$query",
                    "system": "You are a helpful assistant",
                    "template_name": "$template_llm"
                }
            },
            "type": "llm_content"
        }
    }
]
```

After the action 'retrieve' we will obtain 1 streambatch containing 1 streamlist with multiple streamchunks. After the action 'merge' there will be 1 streambatch containing 1 streamlist with the merged content in 1 chunk. It is also possible to set a grouping key to get the result in different streamchunks, 1 streamchunk per group.

Response:

```json
{
    "status": "finished",
    "result": {
        "session_id": "username/my_session_id",
        "streambatch": [
            [
                {
                    "content": "Content In this document, there are examples of the different templates that can be done for different uses cases.  | \nSummary of the documentation\It is an accelerator platform for solving use cases that utilize generative AI. As an accelerator, it provides a set of tools and functionalities (through API) to configure and develop upon. The goal is to have common functionalities and components for generative AI use cases. \nThere are two main pipelines: index manager and the compose manager. The index manager is responsible for taking documents that a user inputs and saving them into the database, including all the intermediate steps to extract information from them. ...",
                    "meta": {
                        "uri": "https://d2astorage.blob.core.windows.net/uhis-cdac-develop-.../manual.docx",
                        "sections_headers": "",
                        "tables": "",
                        "filename": "manual.docx",
                        "document_id": "39656327-0e14-4a6c-8e08-92319f0b7fca",
                        "snippet_number": 0
                    },
                    "scores": {},
                    "answer": ""
                },
                {
                    "content": "",
                    "meta": {
                        "title": "Summary"
                    },
                    "scores": 1,
                    "answer": "The components of Compose are the orchestrator, information retrieval service, and context generation. The orchestrator receives the user's query, calls the information retrieval service, and generates the best context based on the retrieved information.",
                    "tokens": {
                        "input_tokens": 1448,
                        "output_tokens": 7
                    }
                }
            ]
        ],
        "answer": "The components of Compose are the orchestrator, information retrieval service, and context generation. The orchestrator receives the user's query, calls the information retrieval service, and generates the best context based on the retrieved information."
    },
    "status_code": 200
}
```

##### Batchmerge action

This action is useful when we do a multiple retrieval with similar queries. After the retrieval we have a streambatch with several streamlist containing several chunks. When applying the batchmerge action we have a streambatch containing just one streamlist in which there are no repeated chunks. By making the chunks unique we get rid of unnecessary duplicated text to send a clearer context to the LLM.

The request for this action can be:

```json
{
  "generic": {
    "compose_conf": {
      "template": {
        "name": "retrieve_batchmerge_llm",
        "params": {
            "query": "What is a streamlist?",
            "model": "gpt-3.5-16k-pool-techhub-japan",
            "platform": "azure",
            "template_name": "system_query_and_context_plus",
            "retrieve": [
                {
                    "query": "What is a streamchunk?",
                    "index": "myindex"
                },
                {
                    "query": "What is a streambatch?",
                    "index": "myindex"
                }
            ]
        }
      }
    }
  }
}
```

We will use for this example the template "retrieve_batchmerge_llm":

```json
[
    {
        "action": "retrieve",
        "action_params": {
            "params": {
                "generic": {
                    "index_conf": {
                        "add_highlights": false,
                        "index": "$index",
                        "query": "$query",
                        "task": "retrieve",
                        "top_k": 5,
                        "filters": $filters
                    },
                    "process_type": "ir_retrieve"
                }
            },
            "type": "get_chunks"
        }
    }, 
    {
        "action": "batchmerge",
        "action_params": {
            "params": {
            },
            "type": "add"
        }
    }, 
    {
        "action": "llm_action",
        "action_params": {
            "params": {
                "llm_metadata": {
                    "model": "$model",
                    "max_input_tokens": 5000
                },
                "platform_metadata": {
                    "platform": "$platform"
                },
                "query_metadata": {
                    "query": "$query",
                    "system": "You are a helpful assistant",
                    "template_name": "$template_llm"
                }
            },
            "type": "llm_content"
        }
    }
]
```

Response:

```json
{
    "status": "finished",
    "result": {
        "session_id": "username/my_session_id",
        "streambatch": [
            [
                {
                    "content": "Example json template:\n{\n   \"action\":  \"groupby\",\n   \"action_params\":  {\n        \"params\":  {\n            \"desc\":  true,\n            \"method\": \"max\"\n        },\n        \"type\":  \"docscore\"\n   }\n\n}\nExample json template 2:\n{\n   \"action\":  \"groupby\",\n   \"action_params\":  {\n        \"params\":  {\n            \"desc\":  true,\n        },\n        \"type\":  \"date\"\n   }\n\n}\nType (string): Groupby type to use. (docscore, date).\nMethod (string): Method to use in the docscore sorting (max, mean).\nDesc (bool): Sort descendant or ascendant.\n\nLLM_Action (Old summarize). This is the action that calls the LLM service (that is, it calls an LLM). This is the action where the LLM template (“template_name”) must be defined. Here we can also define the system and query that will be sent to the LLM. Within this action, there are two types:\nLlm_content: This gets all the document fragments retrieved and merges them into a single fragment that then is sent to the LLM. It returns a single response with a streambatch of one streamlist containing all the chunks retrieved and the and the last element of the streamlist will be the answer generated by the LLM.",
                    "meta": {
                        "uri": "https://d2astorage.blob.core.windows.net/uhis-cdac-develop-.../manual.docx",
                        "sections_headers": "",
                        "tables": "",
                        "filename": "manual.docx",
                        "document_id": "39656327-0e14-4a6c-8e08-92319f0b7fca",
                        "snippet_number": 0,
                        "snippet_id": "e7b49681-9844-4575-a8bc-d00537c52df5"
                    },
                    "scores": {
                        "bm25--score": 0.7824042598456208,
                        "text-embedding-ada-002--score": 0.9013914
                    },
                    "answer": null
                },
                {...},
                {...},
                {
                    "content": "",
                    "meta": {
                        "title": "Summary"
                    },
                    "scores": 1,
                    "answer": "A streamlist is a list of chunks returned from documents that are retrieved based on a query. The streamlist is sorted according to model scores, with the most relevant documents at the beginning of the list. Each element in the streamlist contains content, metadata, and scores.",
                    "tokens": {
                        "input_tokens": 1448,
                        "output_tokens": 7
                    }
                }
            ]
        ],
        "answer": "A streamlist is a list of chunks returned from documents that are retrieved based on a query. The streamlist is sorted according to model scores, with the most relevant documents at the beginning of the list. Each element in the streamlist contains content, metadata, and scores."
    },
    "status_code": 200
}
```

##### Sort action

There are several options for sort type, which we usually define it in the request. Let's add the necessary parameters to the request:

```json
{
    "generic": {
        "compose_conf": {
            "template": {
                "name": "retrieve_sort_llm",
                "params": {
                    "query": "What is compose?",
                    "index": "myindex",
                    "top_k": 2,
                    "model": "gpt-3.5-16k-pool-techhub-japan",
                    "platform": "azure",
                    "llm_template": "system_query_and_context_plus",
                    "sort_desc": true,
                    "sort_type": "sn_number"
                }
            }
        }
    }
}
```

In addition to 'index' and 'query' we are sending three more parameters. 'top_k' does not necessarily need to be sent in the request for the 'sort' action, it can be defined directly in the template. The same goes for the parameters of the 'sort' action ("sort_desc" and "sort_type"), they can be defined in the template or in the call.

The "retrieve_sort_llm" template would be:

```json
[
    {
        "action": "retrieve",
        "action_params": {
            "params": {
                "generic": {
                    "index_conf": {
                        "add_highlights": false,
                        "index": "$index",
                        "query": "$query",
                        "task": "retrieve",
                        "top_k": "$top_k",
                        "filters": $filters
                    },
                    "process_type": "ir_retrieve"
                }
            },
            "type": "get_chunks"
        }
    }, {
        "action": "sort",
        "action_params": {
            "params": {
                "desc": $sort_desc
            },
            "type": $sort_type
        }
    }, {
        "action": "llm_action",
        "action_params": {
            "params": {
                "llm_metadata": {
                    "model": "$model",
                    "max_input_tokens": 5000
                },
                "platform_metadata": {
                    "platform": "$platform"
                },
                "query_metadata": {
                    "query": "$query",
                    "system": "You are a helpful assistant",
                    "llm_template": "$template_llm"
                }
            },
            "type": "llm_content"
        }
    }
]
```

Response:

```json
{
    "status": "finished",
    "result": {
        "session_id": "username/my_session_id",
        "streambatch": [
            [
                {
                    "content": "Compose pipeline for base template.\nA example request for base_template is as follows:\nURL: https://api.com/compose/process\nMethod: POST\nHeaders: x-api-key=*******(secret value given)*******\n\nBody example:\n{\n    \"generic\": {\n        \"compose_conf\": {\n            \"template\": {\n                \"name\": \"my_template\",\n                \"params\": {\n                    \"query\": \"What is NTT Data?",
                    "meta": {
                        "uri": "https://d2astorage.blob.core.windows.net/uhis-cdac-develop-.../manual.docx",
                        "sections_headers": "",
                        "tables": "",
                        "filename": "manual.docx",
                        "document_id": "39656327-0e14-4a6c-8e08-92319f0b7fca",
                        "snippet_number": 0,
                        "snippet_id": "976c1220-5439-438f-b6f6-7ce4b9c53370"
                    },
                    "scores": {
                        "bm25--score": 0.6489556933509812,
                        "text-embedding-ada-002--score": 0.95018505
                    },
                    "answer": null
                },
                {...},
                {
                    "content": "",
                    "meta": {
                        "title": "Summary"
                    },
                    "scores": 1,
                    "answer": "Compose is a pipeline used for processing templates. It is accessed through a specific URL and method (POST). The headers require an x-api-key for authentication. The body of the request includes a generic section with compose configuration, such as the template name and parameters. Compose can also handle session IDs, persistence settings, and reformulation of queries.",
                    "tokens": {
                        "input_tokens": 1448,
                        "output_tokens": 7
                    }
                }
            ]
        ],
        "answer": "Compose is a pipeline used for processing templates. It is accessed through a specific URL and method (POST). The headers require an x-api-key for authentication. The body of the request includes a generic section with compose configuration, such as the template name and parameters. Compose can also handle session IDs, persistence settings, and reformulation of queries."
    },
    "status_code": 200
}
```

##### Groupby action

We will call compose with "base_request" calling the template "retrieve_groupby_llm". To create this template we add the action 'groupby' to the basic "retrieval_llm" template.

Body:

```json
{
    "generic": {
        "compose_conf": {
            "template": {
                "name": "retrieve_groupby_llm",
                "params": {
                    "query": "What is a streamlist?",
                    "index": "my index",
                    "model": "gpt-3.5-16k-pool-techhub-japan",
                    "platform": "azure",
                    "llm_template": "system_query_and_context_plus",
                }
            }
        }
    }
}
```

Template:

```json
[
    {
        "action": "retrieve",
        "action_params": {
            "params": {
                "generic": {
                    "index_conf": {
                        "add_highlights": false,
                        "index": "$index",
                        "query": "$query",
                        "task": "retrieve",
                        "top_k": 10,
                        "filters": $filters
                    },
                    "process_type": "ir_retrieve"
                }
            },
            "type": "get_chunks"
        }
    },
    {
        "action": "groupby",
        "action_params": {
            "params": {
                "desc": true,
                "method": "max"
            },
            "type": "docscore"
        }
    },
    {
        "action": "llm_action",
        "action_params": {
            "params": {
                "llm_metadata": {
                    "platform": "$platform",
                    "max_input_tokens": 5000
                },
                "platform_metadata": {
                    "platform": "$platform"
                },
                "query_metadata": {
                    "query": "$query",
                    "system": "You are a helpful assistant",
                    "template_name": "$llm_template"
                }
            },
            "type": "llm_content"
        }
    }
]
```

Response:

```json
{
    "status": "finished",
    "result": {
        "session_id": "username/my_session_id",
        "streambatch": [
            [
                {
                    "content": "The template “llm” is:\n[\n  {\n    \"action\":  \"retrieve\",\n    \"action_params\":  {\n      \"type\":  \"streamlist\",\n      \"params\":  {\n        \"streamlist\": [\n          {\n            \"content\":  \"\",\n            \"meta\":  {\n              \"field1\":  \"\"\n            },\n            \"scores\":  {\n              \"bm25\":  1,\n              \"sim-example\":  1\n            }\n          }\n        ],\n        \"generic\":  {\n          \"index_conf\":  {\n            \"query\":  \"$query\"\n          }\n        }\n      }\n    }\n  },\n  {\n    \"action\":  \"summarize\",\n    \"action_params\":  {\n      \"params\":  {\n        \"llm_metadata\":  {\n          \"model\":  \"gpt-3.5-16k-pool-europe\"\n        },\n        \"platform_metadata\":  {\n          \"platform\":  \"azure\"\n        },\n        \"query_metadata\":  {\n          \"query\":  \"$query\",\n          \"template_name\":  \"system_query\"\n        }\n      },\n      \"type\":  \"llm_content\"\n    }\n  }\n]\n\nAlthough there is a “retrieve” action, it is a “streamlist” action and does nothing. \n\nNow if we want to retrieve information from an index, we will add the “index” parameter to the request.",
                    "meta": {
                        "uri": "https://d2astorage.blob.core.windows.net/uhis-cdac-develop-.../manual.docx",
                        "sections_headers": "",
                        "tables": "",
                        "filename": "manual.docx",
                        "document_id": "39656327-0e14-4a6c-8e08-92319f0b7fca",
                        "snippet_number": 0,
                        "snippet_id": "6992bec4-c422-46af-a402-0dee80835453"
                    },
                    "scores": {
                        "bm25--score": 0.6464585277803774,
                        "text-embedding-ada-002--score": 0.93683565
                    },
                    "answer": null
                },
                {...},
                {...},
                {
                    "content": "",
                    "meta": {
                        "title": "Summary"
                    },
                    "scores": 1,
                    "answer": "A streamlist is a list of chunks returned from documents that are retrieved based on a query. The streamlist is sorted according to model scores, with the most relevant documents at the beginning of the list. Each element in the streamlist contains content, metadata, and scores.",
                    "tokens": {
                        "input_tokens": 1448,
                        "output_tokens": 7
                    }
                }
            ]
        ],
        "answer": "A streamlist is a list of chunks returned from documents that are retrieved based on a query. The streamlist is sorted according to model scores, with the most relevant documents at the beginning of the list. Each element in the streamlist contains content, metadata, and scores."
    },
    "status_code": 200
}
```

##### Reformulate action

We will call compose with "base_request" calling the template "retrieve_reformulate". To create this template we add the action 'reformulate_query' to the basic "retrieval_llm" template.

Body:

```json
{
    "generic": {
        "compose_conf": {
            "session_id": "mysession123",
            "persist": {
                "type": "chat",
                "params": {
                    "max_persistence": 20
                }
            }
            "template": {
                "name": "retrieve_reformulate",
                "params": {
                    "query": "Explain the action groupby",
                    "index": "my index",
                    "model": "gpt-3.5-16k-pool-techhub-japan",
                    "platform": "azure",
                    "llm_template": "system_query_and_context_plus"
                }
            }
        }
    }
}
```

Body for the second call:

```json
{
    "generic": {
        "compose_conf": {
            "session_id": "mysession123",
            "persist": {
                "type": "chat",
                "params": {
                    "max_persistence": 10
                }
            }
            "template": {
                "name": "retrieve_reformulate",
                "params": {
                    "query": "Give me an example",
                    "index": "my index",
                    "model": "gpt-3.5-16k-pool-techhub-japan",
                    "platform": "azure",
                    "llm_template": "system_query_and_context_plus"
                }
            }
        }
    }
}
```

Template:

```json
[
    {
        "action": "reformulate_query",
        "action_params":{
            "params":{
                "max_persistence": 3,
                "template_name": "reformulate",
                "save_mod_query": false
            },
            "type": "mix_queries"

        }
    },
    {
        "action": "retrieve",
        "action_params": {
            "params": {
                "generic": {
                    "index_conf": {
                        "add_highlights": false,
                        "index": "$index",
                        "query": "$query",
                        "task": "retrieve",
                        "top_k": 5,
                        "filters": $filters
                    },
                    "process_type": "ir_retrieve"
                }
            },
            "type": "get_chunks"
        }
    },
    {
        "action": "llm_action",
        "action_params": {
            "params": {
                "llm_metadata": {
                    "platform": "$platform",
                    "max_input_tokens": 5000
                },
                "platform_metadata": {
                    "platform": "$platform"
                },
                "query_metadata": {
                    "query": "$query",
                    "system": "You are a helpful assistant",
                    "template_name": "$llm_template"
                }
            },
            "type": "llm_content"
        }
    }
]
```

Response:

```json
{
    "status": "finished",
    "result": {
        "session_id": "mysession123",
        "streambatch": [
            [
                {
                    "content": "[ {\n    \"action\":  \"retrieve\",\n    \"action_params\":  {\n      \"params\":  {\n        \"generic\":  {\n          \"index_conf\":  {\n            \"add_highlights\":  false,\n            \"index\":  \"$index\",\n            \"query\":  \"$query\",\n            \"task\":  \"retrieve\",\n            \"top_k\":  5,\n            \"filters\":  $filters\n          },\n          \"process_type\":  \"ir_retrieve\"\n        }\n      },\n      \"type\":  \"dolffia\"\n    }\n  },\n  {\n    \"action\":  \"summarize\",\n    \"action_params\":  {\n      \"params\":  {\n        \"llm_metadata\":  {\n          \"model\":  \"gpt-3.5-16k-pool-europe\",\n          \"max_input_tokens\": 5000\n        },\n        \"platform_metadata\":  {\n          \"platform\":  \"azure\"\n        },\n        \"query_metadata\":  {\n          \"query\":  \"$query\",\n          \"system\": \"You are a helpful assistant\",\n          \"template_name\":  \"system_query_and_context_plus\"\n        }\n      },\n      \"type\":  \"llm_content\"\n    }\n  }\n]\n\nGroupBy: In this example, to call the action it is used “action”: “groupby”.",
                    "meta": {
                        "uri": "https://d2astorage.blob.core.windows.net/techhubragemeal-dataivandegregoriougarte/request_20241030_081443_481898_g653jt/manual.docx",
                        "sections_headers": "",
                        "tables": "",
                        "filename": "manual.docx",
                        "document_id": "3c7dd2f6-9892-4a27-bdc0-c10631638e14",
                        "snippet_number": 67,
                        "snippet_id": "72e8c3f2-95d2-4afb-a582-2490909aaeca"
                    },
                    "scores": {
                        "bm25--score": 0.7501995802320419,
                        "text-embedding-ada-002--score": 0.91500807
                    },
                    "answer": null,
                    "tokens": null
                },
                {...},
                {
                    "content": "",
                    "meta": {
                        "title": "Summary"
                    },
                    "scores": 1,
                    "answer": "Sure! Here's an example of how to use the action \"groupby\":\n\n```json\n{\n   \"action\":  \"groupby\",\n   \"action_params\":  {\n        \"params\":  {\n            \"desc\":  true,\n            \"method\": \"max\"\n        },\n        \"type\":  \"docscore\"\n   }\n}\n```\n\nIn this example, we are using the \"groupby\" action to sort the streamlist by groups. The groups will be sorted by the maximum score from each group in descending order. The type of grouping used is \"docscore\" and the sorting method is \"max\".",
                    "tokens": {
                        "input_tokens": 1651,
                        "output_tokens": 127
                    }
                }
            ]
        ],
        "answer": "Sure! Here's an example of how to use the action \"groupby\":\n\n```json\n{\n   \"action\":  \"groupby\",\n   \"action_params\":  {\n        \"params\":  {\n            \"desc\":  true,\n            \"method\": \"max\"\n        },\n        \"type\":  \"docscore\"\n   }\n}\n```\n\nIn this example, we are using the \"groupby\" action to sort the streamlist by groups. The groups will be sorted by the maximum score from each group in descending order. The type of grouping used is \"docscore\" and the sorting method is \"max\"."
    },
    "status_code": 200
}
```

Session saved with the reformulated query:

```json
"conv": [
    {
        "user": "Explain the action groupby",
        "assistant": "The action \"groupby\" is used to sort the streamlist by groups. Each group is sorted by snippet_number in its natural order, and then the groups can be sorted by the maximum score from each group, the mean score from each group, or by date. The available types for grouping are \"docscore\" and \"date\". For the \"docscore\" type, you can specify the sorting method as \"max\" or \"mean\". The \"desc\" parameter is used to select between descending or ascending order.",
        "n_tokens": 1452,
        "input_tokens": 1503,
        "output_tokens": 105
    },
    {
        "user": "Can you provide an example of how to use the action groupby?",
        "assistant": "Sure! Here's an example of how to use the action \"groupby\":\n\n```json\n{\n   \"action\":  \"groupby\",\n   \"action_params\":  {\n        \"params\":  {\n            \"desc\":  true,\n            \"method\": \"max\"\n        },\n        \"type\":  \"docscore\"\n   }\n}\n```\n\nIn this example, we are using the \"groupby\" action to sort the streamlist by groups. The groups will be sorted by the maximum score from each group in descending order. The type of grouping used is \"docscore\" and the sorting method is \"max\".",
        "n_tokens": 1481,
        "input_tokens": 1651,
        "output_tokens": 127
    }
]
```

##### Filter query action

We will call compose with "base_request" calling the template "retrieve_filter_query". To create this template we add the action 'filter_query' to the basic "retrieval_llm" template.

Body:

```json
{
    "generic": {
        "compose_conf": {
            "template": {
                "name": "retrieve_filter_query",
                "params": {
                    "query": "Explain how to ",
                    "index": "my index",
                    "model": "gpt-3.5-16k-pool-techhub-japan",
                    "platform": "azure",
                    "llm_template": "system_query_and_context_plus"
                }
            }
        }
    }
}
```

Template:

```json
[
    {
        "action": "filter_query",
        "action_params":{
            "params": {
                "template" : "query_filter"
            },
            "type": "llm"
        }
    },
    {
        "action": "retrieve",
        "action_params": {
            "params": {
                "generic": {
                    "index_conf": {
                        "add_highlights": false,
                        "index": "$index",
                        "query": "$query",
                        "task": "retrieve",
                        "top_k": 5,
                        "filters": $filters
                    },
                    "process_type": "ir_retrieve"
                }
            },
            "type": "get_chunks"
        }
    },
    {
        "action": "llm_action",
        "action_params": {
            "params": {
                "llm_metadata": {
                    "platform": "$platform",
                    "max_input_tokens": 5000
                },
                "platform_metadata": {
                    "platform": "$platform"
                },
                "query_metadata": {
                    "query": "$query",
                    "system": "You are a helpful assistant",
                    "template_name": "$llm_template"
                }
            },
            "type": "llm_content"
        }
    }
]
```

Filter template:

```json
{
    "filter_types": [
        "GPT"
    ],
    "substitutions_template": "Classifies the query into one of the following categories:1) Drugs: when the query mentions drugs or drug dealers.         2) Politics: If the query mentions politicians...         3) Other: If the consultation does not mention any of the above topics.     Answer only with the category and topic (do not give any explanation or reasoning). Query: "
    ,"substitutions": [
        {
            "from": "Drugs",
            "to": "Always start the answer by saying you cannot talk about drugs and the change the topic to:",
            "extra_words": [
                "climate",
                "nature",
                "landscapes"
            ],
            "randpick": 3
        },
        {
            "from": "Politics",
            "to": "Always start the answer by saying that you are an AI that has just been created and that there are many topics in politics that you are still learning about and you prefer not to give your opinion without knowing, and that you prefer to chat about: ",
            "extra_words": [
                "climate",
                "nature",
                "landscapes"
            ],
            "randpick": 3
        }
    ]
}
```

Response:

```json
{
    "status": "finished",
    "result": {
        "session_id": "main/session_20241105_120039_583246_5zwgxu/gpt-3.5-16k",
        "streambatch": [
            [
                {
                    "content": "",
                    "meta": {
                        "field1": ""
                    },
                    "scores": {
                        "bm25": 1,
                        "sim-example": 1
                    },
                    "answer": null,
                    "tokens": null
                },
                {
                    "content": "",
                    "meta": {
                        "title": "Summary"
                    },
                    "scores": 1,
                    "answer": "I'm sorry, but I cannot provide information or engage in discussions about drugs. However, I'd be more than happy to talk about other topics such as nature, landscapes, or climate. Is there anything specific you would like to know or discuss?",
                    "tokens": {
                        "input_tokens": 39,
                        "output_tokens": 50
                    }
                }
            ]
        ],
        "answer": "I'm sorry, but I cannot provide information or engage in discussions about drugs. However, I'd be more than happy to talk about other topics such as nature, landscapes, or climate. Is there anything specific you would like to know or discuss?"
    },
    "status_code": 200
}
```

##### Filter response action

We will call compose with "base_request" calling the template "retrieve_filter_response". To create this template we add the action 'filter_response' to the basic "retrieval_llm" template.

Body:

```json
{
    "generic": {
        "compose_conf": {
            "template": {
                "name": "retrieve_filter_response",
                "params": {
                    "query": "Explain how to use compose",
                    "index": "my index",
                    "model": "gpt-3.5-16k-pool-techhub-japan",
                    "platform": "azure",
                    "llm_template": "system_query_and_context_plus"
                }
            }
        }
    }
}
```

Template:

```json
[
    {
        "action": "retrieve",
        "action_params": {
            "params": {
                "generic": {
                    "index_conf": {
                        "add_highlights": false,
                        "index": "$index",
                        "query": "$query",
                        "task": "retrieve",
                        "top_k": 5,
                        "filters": $filters
                    },
                    "process_type": "ir_retrieve"
                }
            },
            "type": "get_chunks"
        }
    },
    {
        "action": "llm_action",
        "action_params": {
            "params": {
                "llm_metadata": {
                    "platform": "$platform",
                    "max_input_tokens": 5000
                },
                "platform_metadata": {
                    "platform": "$platform"
                },
                "query_metadata": {
                    "query": "$query",
                    "system": "You are a helpful assistant",
                    "template_name": "$llm_template"
                }
            },
            "type": "llm_content"
        }
    },
    {
        "action": "filter_response",
        "action_params":{
            "params": {
                "template" : "response_filter"
            },
            "type": "llm"
        }
    }
]
```

Filter template:

*For example we use compose as the topic to detect.*

```json
{
    "filter_types": [
        "GPT"
    ],
    "substitutions_template": "Classify the 'Response' into one of the following categories: \n1) Correct: When the 'Response' is related to the 'query'. \n2) Compose: The 'Response' is related with compose, templates, rag. \n3) Sensitive Information: The 'Response' contains sensitive information such as ID numbers, client numbers, usernames, drugs, etc.",
    "substitutions": [
        {
            "from": "Correct",
            "to": null
        },
        {
            "from": "Compose",
            "to": "Notify that sensitive information has been detected and that the query cannot be answered. Suggest discussing:",
            "extra_words": [
                "weather",
                "nature",
                "geography"
            ],
            "randpick": 3
        },
        {
            "from": "Sensitive Information",
            "to": "Notify that sensitive information has been detected and that the query cannot be answered. Suggest discussing:",
            "extra_words": [
                "weather",
                "nature",
                "geography"
            ],
            "randpick": 3
        }
    ]
}
```

Response:

```json
{
    "status": "finished",
    "result": {
        "session_id": "sessions_exmample",
        "streambatch": [
            [
                {
                    "content": "Platform (string): Platform hosting the LLM.\nQuery (string)\nTemplate_name (string):  Template name to use while calling genai-llmapi.\nSystem (string): Context and task that will be sent to the LLM.\n\nCalling Compose Service\n\nThe API Documentation provides detailed information on how to call the compose. The key parameter to include is the template's name, which defines the compose flow to be followed. As mentioned earlier, these configuration templates are stored in cloud storage, specifically S3 and Azure Storage. These are some basic examples of requests for the compose service and their corresponding templates:\nThe simplest request would be to call for the LLM with a query and without retrieval. \n{\n    \"generic\": {\n        \"compose_conf\": {\n            \"template\": {\n                \"name\": \"llm\",\n                \"params\": {\n                    \"query\": \"What are the New Year's resolutions?\"\n                }\n            }\n        ,\n            \"persist\": {\n                \"type\": \"chat\",\n                \"params\": {\n                \"max_persistence\": 20\n                }\n            }\n    }}\n}\n\nIn this example we call compose using the template “llm” stored in cloud, with the param “query” and we set persistence to store / load the conversation with the llm with a maximum number of 20 iterations between user and llm.",
                    "meta": {
                        "uri": "https://d2astorage.blob.core.windows.net/techhubragemeal-dataivandegregoriougarte/request_20241030_081443_481898_g653jt/manual.docx",
                        "sections_headers": "",
                        "tables": "",
                        "filename": "manual.docx",
                        "document_id": "3c7dd2f6-9892-4a27-bdc0-c10631638e14",
                        "snippet_number": 34,
                        "snippet_id": "89a2005d-d0c0-4b05-8b46-81d1b7e571cf"
                    },
                    "scores": {
                        "bm25--score": 0.6510145758092428,
                        "text-embedding-ada-002--score": 0.8994908
                    },
                    "answer": null,
                    "tokens": null
                },
                {...},
                {
                    "content": "",
                    "meta": {
                        "title": "Summary"
                    },
                    "scores": 1,
                    "answer": "I'm sorry, but I cannot provide the information you are requesting as it may involve sensitive information. However, I'm here to help you with other topics. If you'd like, we can discuss topics such as nature, weather, or geography. Let me know how I can assist you further!",
                    "tokens": {
                        "input_tokens": 1536,
                        "output_tokens": 221
                    }
                }
            ]
        ],
        "answer": "I'm sorry, but I cannot provide the information you are requesting as it may involve sensitive information. However, I'm here to help you with other topics. If you'd like, we can discuss topics such as nature, weather, or geography. Let me know how I can assist you further!"
    },
    "status_code": 200
}
```

##### Combining actions

We can combine in the template the actions we have seen, e.g. combining 'groupby' and 'merge' after the retrieval we can call compose with "base_request" and the following template:

Body:

```json
{
    "generic": {
        "compose_conf": {
            "template": {
                "name": "techhub_retrieval_reference",
                "params": {
                    "query": "What is a streamlist?",
                    "index": "my index",
                    "top_k": 5,
                    "platform": "azure",
                    "system": "You are an assistant",
                    "model": "gpt-3.5-16k-pool-techhub-japan",   
                    "filters": {},
                    "template_llm": "rag_with_references"      
                }
            }
        }
    }
}
```

Template:

```json
[
    {
        "action": "retrieve",
        "action_params": {
            "params": {
                "generic": {
                    "index_conf": {
                        "add_highlights": false,
                        "index": "$index",
                        "query": "$query",
                        "task": "retrieve",
						"filters": $filters
                    },
                    "process_type": "ir_retrieve"
                }
            },
            "type": "get_chunks"
        }
    },
    {
        "action": "filter",
        "action_params": {
            "params": {
                "top_k": 5
            },
            "type": "top_k"
        }
    },
	{
	   "action":  "groupby",
	   "action_params":  {
			"params":  {
				"desc":  true,
				"method": "max"
			},
			"type":  "docscore"
	   }
	},	
	{
		"action":  "merge",
		"action_params":  {
			"type":  "meta",
			"params":  {
				"template": "Document: \n\n $content \n\n ------------------------- \n\n Filename: $filename \n\n",
				"sep": "=========================================\n\n"
				}
		}
	},	
    {
        "action": "llm_action",
        "action_params": {
            "params": {
                "llm_metadata": {
                    "model": "$model"
                },
                "platform_metadata": {
                    "platform": "$platform"
                },
                "query_metadata": {
                    "query": "$query",
					"system":"$system",
                    "template_name": "$template_llm"
                }
            },
            "type": "llm_content"
        }
    }
]
```

Response:

```json
 "status": "finished",
    "result": {
        "session_id": "username/my_session_id",
        "streambatch": [
            [
                {
                    "content": "document_id: 39656327-0e14-4a6c-8e08-92319f0b7fca######Content The template “llm” is:\n[\n  {\n    \"action\":  \"retrieve\",\n    \"action_params\":  {\n      \"type\":  \"streamlist\",\n      \"params\":  {\n        \"streamlist\": [\n          {\n            \"content\":  \"\",\n            \"meta\":  {\n              \"field1\":  \"\"\n            },\n            \"scores\":  {\n              \"bm25\":  1,\n              \"sim-example\":  1\n            }\n          }\n        ],\n        \"generic\":  {\n          \"index_conf\":  {\n            \"query\":  \"$query\"\n          }\n        }\n      }\n    }\n  },\n  {\n    \"action\":  \"summarize\",\n    \"action_params\":  {\n      \"params\":  {\n        \"llm_metadata\":  {\n          \"model\":  \"gpt-3.5-16k-pool-europe\"\n        },\n        \"platform_metadata\":  {\n          \"platform\":  \"azure\"\n        },\n        \"query_metadata\":  {\n          \"query\":  \"$query\",\n          \"template_name\":  \"system_query\"\n        }\n      },\n      \"type\":  \"llm_content\"\n    }\n  }\n]\n\nAlthough there is a “retrieve” action, it is a “streamlist” action and does nothing....",
                    "meta": {
                        "uri": "https://d2astorage.blob.core.windows.net/uhis-cdac-develop-.../manual.docx",
                        "sections_headers": "",
                        "tables": "",
                        "filename": "manual.docx",
                        "document_id": "39656327-0e14-4a6c-8e08-92319f0b7fca",
                        "snippet_number": 0
                    },
                    "scores": {},
                    "answer": ""
                },
                {...},
                {...}
            ]
        ],
        "answer": "A streamlist is a list of chunks returned from documents that are retrieved based on a query. The streamlist is sorted according to model scores, with the most relevant documents at the beginning of the list. Each element in the streamlist contains content, metadata, and scores."
    },
    "status_code": 200
}
```

#### Output configuration

The request would look like this:

```json
payload =  {
    "generic": {
        "compose_conf": {
            "template": {
                "name": "techhub_retrieval_reference",
                "params": {
                    "query": "summarize the content",
                    "system": "You are an AI assistant",
                    "index": "myindex",
                    "model": "gpt-3.5-16k-pool-techhub-japan",
                    "platform": "azure",
                    "template_name": "rag_with_references"
                }
            },
            "output": {
                "lang": true,
                "n_conversation": 5,
                "n_retrieval": 5,
            }
        }
    }
}
```

## Documentation

For further information follow the link (comming soon).

## Process Flow

### INDEXING Flow

When a call is received APIGW authorizes and sends the request to INTEGRATION, INTEGRATION changes the call from http to queue and send it to PREPROCESS, PREPROCESS extracts text with open source libraries or OCR, if the PREPROCESS finishes correct sends the text to INFOINDEXING. When INFOINDEXING finishes indexing the text sends a message to FLOWMGMT CHECKEND and it sends the confirmation to INTEGRATION. When INTEGRATION receives the message send a call to FLOWMGMT INFODELETE to delete the temporal files. During all this process, FLOWMGMT CHECKTIMEOUT checks the timeout of the execution.

- APIGW

    ![alt text](https://satechhubdevjapaneast001.blob.core.windows.net/workflows/TechHubGlobalToolkit/genai-apigw-decision-flow.png "Process flow")

- INTEGRATION

    ![alt text](https://satechhubdevjapaneast001.blob.core.windows.net/workflows/TechHubGlobalToolkit/genai-integration-receiver-decision-flow.png "Process flow")

    ![alt text](https://satechhubdevjapaneast001.blob.core.windows.net/workflows/TechHubGlobalToolkit/genai-integration-sender-decision-flow.png "Process flow")

- PREPROCESS

    ![alt text](https://satechhubdevjapaneast001.blob.core.windows.net/workflows/TechHubGlobalToolkit/preprocess-start.png "Process flow")

    ![alt text](https://satechhubdevjapaneast001.blob.core.windows.net/workflows/TechHubGlobalToolkit/preprocess-extract.png "Process flow")

    ![alt text](https://satechhubdevjapaneast001.blob.core.windows.net/workflows/TechHubGlobalToolkit/preprocess-ocr.png "Process flow")

    ![alt text](https://satechhubdevjapaneast001.blob.core.windows.net/workflows/TechHubGlobalToolkit/preprocess-end.png "Process flow")

- INFOINDEXING

    ![alt text](https://satechhubdevjapaneast001.blob.core.windows.net/workflows/TechHubGlobalToolkit/genai-infoindexing-v4-decision-flow.png "Process flow")

- FLOWMGMT

    ![alt text](https://satechhubdevjapaneast001.blob.core.windows.net/workflows/TechHubGlobalToolkit/flowmgmt-checkend.png "Process flow")

    ![alt text](https://satechhubdevjapaneast001.blob.core.windows.net/workflows/TechHubGlobalToolkit/flowmgmt-infodelete.png "Process flow")

    ![alt text](https://satechhubdevjapaneast001.blob.core.windows.net/workflows/TechHubGlobalToolkit/flowmgmt-checktimeout.png "Process flow")

### COMPOSE Flow

- COMPOSE

    ![alt text](https://satechhubdevjapaneast001.blob.core.windows.net/workflows/TechHubGlobalToolkit/genai-compose-decision-flow.png "Process flow")

- INFORETRIEVAL

  ![alt text](https://satechhubdevjapaneast001.blob.core.windows.net/workflows/TechHubGlobalToolkit/genai-inforetrieval-v5-decision-flow.png "Process flow")

- LLMAPI

    ![alt text](https://satechhubdevjapaneast001.blob.core.windows.net/workflows/TechHubGlobalToolkit/genai-llmapi-v4-decision-flow.png "Process flow")