### This code is property of the GGAO ###


# Native imports
import json
from tempfile import TemporaryFile
import copy
import time
import os

# Custom imports
from common.deployment_utils import BaseDeployment
from common.genai_controllers import storage_containers, db_dbs, set_queue, set_storage, set_db
from common.genai_controllers import get_sizes, bytes_mode, download_batch_files_async, upload_batch_files_async
from common.genai_status_control import update_status
from common.genai_json_parser import *
from common.preprocess.preprocess_ocr import *
from common.preprocess.preprocess_utils import format_indexing_metadata
from common.preprocess.preprocess_utils import get_language
from common.status_codes import *
from common.services import *
from common.error_messages import *
from common.utils import remove_local_files, resize_image, get_image_size
from genai_sdk_services.resources.vision.BaseOCR import LLMOCR


class PreprocessOCRDeployment(BaseDeployment):

    def __init__(self):
        """ Creates the deployment"""
        super().__init__()
        set_queue(self.Q_IN)
        set_storage(storage_containers)
        set_db(db_dbs)

        self.PERIOD = 60

    @property
    def must_continue(self) -> bool:
        """ True if the output should be sent to next step """
        return True

    @property
    def service_name(self) -> str:
        """ Service name.
        The name must be the same as the AWS SQS queue name without the Q_ identifier.
        Example: Q_TRAIN_GPU - train_gpu
        It can be in lowercase but must have the same chars.
        The endpoint for sync deployments will be the same as the service name. """
        return PREPROCESS_OCR_SERVICE

    @property
    def max_num_queue(self) -> int:
        """ Max number of messages to read from queue at once """
        return 1

    def merge_files_text(self, upload_docs: dict, path_file_txt: str, path_file_text: str, filename: str, num_pags: int, metadata: dict):
        """Merge text by pages and add metadata to the file

        :param upload_docs: Information of path with files to extract in OCR
        :param path_file_txt: Path to write the file txt
        :param path_file_text: Path to write the file text
        :param filename: Name of the file
        :param num_pags: Number of pages
        :param metadata: Metadata to add in the file
        """
        # Create the property directories
        os.makedirs(os.path.dirname(path_file_text), exist_ok=True)
        os.makedirs(os.path.dirname(path_file_txt), exist_ok=True)

        self.logger.info("Merging text by pages and adding metadata to the file")
        # Read all pages generated by OCR
        with TemporaryFile("w+", encoding="utf-8") as temp:
            for text_by_pag in upload_docs['text']:
                with open(text_by_pag[1], "r", encoding="utf-8") as r:
                    temp.write(r.read())

            # Create file text with all pages and format to indexing

            with open(path_file_text, "a", encoding="utf-8") as w:
                temp.flush()
                temp.seek(0)
                w.write(format_indexing_metadata(temp.read(), filename, num_pags, metadata))

            # Create file txt with all pages and format to indexing
            with open(path_file_txt, "a", encoding="utf-8") as w:
                temp.flush()
                temp.seek(0)
                w.write(format_indexing_metadata(temp.read(), filename, num_pags, metadata))

    def process(self, json_input: dict) -> Tuple[bool, dict, str]:
        """ Main function. Return if the output must be written to next step, the output to write and the next step
        :return: Tuple[bool, dict, str]
        """
        self.logger.debug(f"Data entry: {json_input}")
        message = json_input
        origin_txt_path = ""
        filename = ""
        next_service = PREPROCESS_END_SERVICE
        msg = json.dumps({'status': ERROR, 'msg': "Error while extracting text in OCR"})
        redis_status = db_dbs['status']
        dataset_status_key = get_dataset_status_key(json_input=json_input)

        try:
            try:
                generic = get_generic(json_input)
                specific = get_specific(json_input)
            except KeyError:
                self.logger.error("[Process] Error parsing JSON. No generic and specific configuration", exc_info=get_exc_info())
                raise Exception(PARSING_PARAMETERS_ERROR)

            try:
                project_conf = get_project_config(generic=generic)
                process_id = project_conf['process_id']
                process_type = project_conf['process_type']
                url = project_conf['report_url']
                department = project_conf['department']
                tenant = project_conf['tenant']
            except KeyError:
                self.logger.error(f"[Process {dataset_status_key}] Error parsing JSON. No configuration of project defined", exc_info=get_exc_info())
                raise Exception(PARSING_PARAMETERS_ERROR)

            try:
                workspace = storage_containers['workspace']
            except KeyError:
                self.logger.error(f"[Process {dataset_status_key}] Error parsing JSON. No credentials OCR defined", exc_info=get_exc_info())
                raise Exception(GETTING_CREDENTIALS_OCR_ERROR)

            try:
                document = get_document(specific=specific)
                num_pags = document['n_pags']
                filename = document['filename']
                language = document['language']
                metadata = get_metadata_conf(json_input)
                file_folder = os.path.splitext(filename)[0]
                file_folder_txt = f"{file_folder}.txt"
                merge_filename = f"{os.path.splitext(os.path.basename(filename))[0]}.txt"
            except KeyError:
                self.logger.error(f"[Process {dataset_status_key}] Error getting params specific of documents", exc_info=get_exc_info())
                raise Exception(GETTING_DOCUMENTS_PARAMS_ERROR)

            try:
                ocr_conf = get_ocr_config(generic=generic)
                ocr_files_size = ocr_conf['files_size']
                ocr_batch_length = ocr_conf['batch_length']
                ocr_calls_per_minute = ocr_conf['calls_per_minute']
                ocr = ocr_conf['ocr']
                # Adding headers to llm_ocr_conf
                if ocr == "llm-ocr":
                    llm_ocr_conf = copy.deepcopy(ocr_conf.get('llm_ocr_conf', {}))
                    llm_ocr_conf['headers'] = {
                        "x-department": department,
                        "x-tenant": tenant,
                        "x-reporting": url
                    }
                    llm_ocr_conf['language'] = "en" if language =="default" else language
            except KeyError:
                self.logger.error(f"[Process {dataset_status_key}] Error parsing JSON. No configuration ocr defined", exc_info=get_exc_info())
                raise Exception(GETTING_CREDENTIALS_OCR_ERROR)

            try:
                path_images = specific['paths']['images']
                origin_img_path = specific['path_img']
                origin_cells_path = specific['path_cells']
                origin_text_path = specific['path_text']
                origin_txt_path = specific['path_txt']
                origin_tables_path = specific['path_tables']
            except KeyError:
                self.logger.error(f"[Process {dataset_status_key}] Error parsing JSON. No configuration specific defined", exc_info=get_exc_info())
                raise Exception(GETTING_PATHS_ERROR)

            try:
                do_cells_ocr = get_do_cells_ocr(generic=generic)
                do_lines_ocr = get_do_lines_ocr(generic=generic)
                do_tables = ocr_conf.get('extract_tables', False)
            except KeyError:
                self.logger.error(f"[Process {dataset_status_key}] Error getting params to extract cells or lines", exc_info=get_exc_info())
                raise Exception(GETTING_LINES_AND_CELLS_ERROR)

            path_images.sort(key=lambda x: x['number'])
            files = [image['filename'] for image in path_images]

            try:
                start_time = time.time()  
                logger.debug("Downloading images from storage.")
                normalized_files = [file.replace("\\", "/") for file in files]
                download_batch_files_async(workspace, normalized_files, local_directory=os.path.dirname(normalized_files[0]))
                end_time = time.time() 
                logger.info(f"Time taken for downloading images: {end_time - start_time} seconds.")
                if ocr == "llm-ocr":
                    logger.info("Images will be resized by LLM as llm-ocr model has been selected")
                else:
                    logger.info("Resizing images if is necesary.")
                    resized_files = []
                    for file in normalized_files:
                        _, resized = resize_image(file)
                        if resized:
                            resized_files.append(file)
                    
                    if resized_files:
                        remote_directory = os.path.dirname(resized_files[0])
                        upload_start_time = time.time()
                        upload_batch_files_async(workspace, resized_files, remote_directory)
                        upload_end_time = time.time()
                        logger.info(f"Time taken for uploading resized images: {upload_end_time - upload_start_time:.2f} seconds.")

            except Exception:
                self.logger.error(f"[Process {dataset_status_key}] Error resizing images", exc_info=get_exc_info())
                raise Exception(RESIZING_IMAGE_ERROR)

            try:
                file_sizes = []
                for file in normalized_files:
                    file_sizes.append(get_image_size(file))
            except Exception:
                self.logger.error(f"[Process {dataset_status_key}] Error getting sizes of images from storage", exc_info=get_exc_info())
                raise Exception(GETTING_FILES_SIZE_ERROR)

            try:
                upload_docs = {'text': [], 'cells': [], 'paragraphs': [], 'words': [], 'tables': [], 'lines': [], 'txt': []}
                prefix_map = {'images': origin_img_path, 'text': origin_text_path, 'cells': origin_cells_path, 'tables': origin_tables_path, 'txt': origin_txt_path}

                path_file_txt = os.path.join(prefix_map['txt'], file_folder_txt).replace("\\", "/")
                path_file_text = os.path.join(prefix_map['text'], "ocr", file_folder, merge_filename).replace("\\", "/")

                if ocr == "llm-ocr" and LLMOCR.get_queue_mode():
                    try:
                        extract_docs = get_ocr_files(files, ocr, prefix_map, do_cells_ocr, do_tables, do_lines_ocr, bytes_mode, **{"llm_ocr_conf":llm_ocr_conf})
                        for key, value in extract_docs.items():
                            if key in upload_docs:
                                upload_docs[key] += value
                    except Exception as e:
                        self.logger.error(f"[Process {dataset_status_key}] Error while writing in LLM OCR queue", exc_info=get_exc_info())
                        self.logger.error(f"[Process {dataset_status_key}] LLM OCR specific error message: {str(e)}")
                        raise Exception(LLM_OCR_ERROR)
                else:
                    for batch_idx, batch in enumerate(chunk(files, file_sizes, ocr_files_size, ocr_batch_length)):
                        self.logger.debug(f"Batch {batch_idx} of size {sum_up_size([x['size'] for x in batch])}MiB containing {len(batch)} files: {[x['filename'] for x in batch]}.")

                        # OCR rate control for Google Cloud so now unused
                        #self.logger.debug("Calculating requests by size and to send ocr.")
                        #count = len(batch)
                        #ocr_rates = {}
                        #requests = ocr_rates.get(ocr, [])
                        #requests = insert_at_rate(requests, count, ocr_calls_per_minute, self.PERIOD)
                        #ocr_rates[ocr] = requests

                        files_to_process = [image['filename'] for image in batch]
                        if ocr == "llm-ocr":
                            extract_docs = get_ocr_files(files_to_process, ocr, prefix_map, do_cells_ocr, do_tables, do_lines_ocr, bytes_mode, **{"llm_ocr_conf":llm_ocr_conf})
                        else:
                            extract_docs = get_ocr_files(files_to_process, ocr, prefix_map, do_cells_ocr, do_tables, do_lines_ocr, bytes_mode)
                        for key, value in extract_docs.items():
                            if key in upload_docs:
                                upload_docs[key] += value
            except Exception:
                self.logger.error(f"[Process {dataset_status_key}] Error where extract files with ocr", exc_info=get_exc_info())
                raise Exception(EXTRACTING_TEXT_OCR_ERROR)

            try:
                resource = f"{process_type}/ocr/pages"
                self.report_api(num_pags, dataset_status_key, url, resource, process_id, "PAGS")
            except Exception:
                self.logger.warning(f"[Process {dataset_status_key}] Error reporting ocr pages.", exc_info=get_exc_info())

            try:
                self.logger.info("Merging files of text.")
                self.merge_files_text(upload_docs, path_file_txt, path_file_text, filename, num_pags, metadata)
            except Exception:
                self.logger.error(f"[Process {dataset_status_key}] Error merging files of text", exc_info=get_exc_info())
                raise Exception(MERGING_FILES_ERROR)

            try:
                self.logger.info("Uploading files to storage.")
                upload_docs['text'].append((path_file_text, path_file_text))
                #upload_docs['txt'].append((path_file_txt, path_file_txt))

                upload_docs['text'].append((path_file_text, path_file_text))

                total_upload_time = 0
                for key, paths in upload_docs.items():
                    if key == 'txt':
                        continue
                    if ocr == "llm-ocr" and key not in ["text"]: 
                        continue # In llm-ocr cells paragraphs words tables and lines are not extracted 
        
                    if paths:
                        files_by_directory = {}
                        for remote_path, local_path in paths:
                            if '/txt/' in remote_path or remote_path.startswith('txt/'):
                                continue

                            remote_dir = os.path.dirname(remote_path)
                            if remote_dir not in files_by_directory:
                                files_by_directory[remote_dir] = []
                            files_by_directory[remote_dir].append((local_path, remote_path))
                        
                        for remote_dir, local_files in files_by_directory.items():
                            if not local_files: 
                                continue
                            
                            local_file_paths = [local_path for local_path, _ in local_files]

                            upload_start_time = time.time()
                            upload_batch_files_async(workspace, local_file_paths, remote_dir)
                            upload_end_time = time.time()
                            upload_time = upload_end_time - upload_start_time
                            total_upload_time += upload_time
                            self.logger.info(f"Time taken for uploading {len(local_file_paths)} files to {remote_dir}: {upload_time:.2f} seconds.")
            except Exception:
                self.logger.error(f"[Process {dataset_status_key}] Error uploading files to storage", exc_info=get_exc_info())
                raise Exception(UPLOADING_FILES_ERROR)

            try:
                with open(path_file_txt, encoding='utf-8') as f:
                    language = get_language(f.read())
            except Exception:
                self.logger.warning(f"[Process {dataset_status_key}] Error getting languages. Assign language by default", exc_info=get_exc_info())
                language = "*"

            specific['paths']['text'] = f"{prefix_map['txt']}/{os.path.splitext(filename)[0]}.txt"
            document['language'] = language

            msg = json.dumps({'status': DOCUMENT_PROCESSED, 'msg': "Document processed successfully"})
        except Exception as ex:
            dataset_status_key = get_dataset_status_key(json_input=json_input)
            next_service = PREPROCESS_END_SERVICE
            self.logger.error(f"[Process {dataset_status_key}] Error in preprocess ocr.", exc_info=get_exc_info())
            msg = json.dumps({'status': ERROR, 'msg': str(ex)})

        if origin_txt_path and filename:
            # Remove local files
            self.logger.info("Deleting local files temporary.")
            try:
                remove_local_files(origin_txt_path)
            except Exception:
                self.logger.warning(f"[Process {dataset_status_key}] Error while deleting file {filename}.")

        update_status(redis_status, dataset_status_key, msg)
        return self.must_continue, message, next_service


if __name__ == "__main__":
    deploy = PreprocessOCRDeployment()
    deploy.async_deployment()
